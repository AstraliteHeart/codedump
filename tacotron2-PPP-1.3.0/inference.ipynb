{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Start) Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nWhat the fuck did you just fucking say about me, you little pony?', 'I’ll have you know I graduated top of my class in magic kindergarten, and I’ve been involved in numerous secret raids on Nightmare Moon, and I have over 300 confirmed friendships.', 'I am trained in magic warfare and I’m the top pony in the entire Equestrian armed forces.', 'You are nothing to me but just another friend.', 'I will wipe you the fuck out with friendship the likes of which has never been seen before on Equestria, mark my fucking words.', 'You think you can get away with saying that shit to me over the Ponynet?', 'Think again, fucker.', 'As we speak I am contacting my secret network of pegasi across Equestria and your hoofprints are being traced right now so you better prepare for the storm, maggot.', 'The storm that wipes out the pathetic little thing you call your life.', 'You’re fucking dead, pony.', 'I can be anywhere, anytime, and I can hug you in over seven hundred ways, and that’s just with my bare hooves.', 'Not only am I extensively trained in unarmed friendship, but I have access to the entire arsenal of ponies and I will use it to its full extent to wipe your miserable flank off the face of the continent, you little pony.', 'If only you could have known what magical friendship your little “clever” comment was about to bring down upon you, maybe you would have held your fucking tongue.', 'But you couldn’t, you didn’t, and now you’re paying the price, you goddamn pony.', 'I will shit friendship all over you and you will drown in it.', 'You’re fucking dead, pony.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text = \"\"\"\n",
    "What the fuck did you just fucking say about me, you little pony?\n",
    "I’ll have you know I graduated top of my class in magic kindergarten, and I’ve been involved in numerous secret raids on Nightmare Moon, and I have over 300 confirmed friendships.\n",
    "I am trained in magic warfare and I’m the top pony in the entire Equestrian armed forces.\n",
    "You are nothing to me but just another friend.\n",
    "I will wipe you the fuck out with friendship the likes of which has never been seen before on Equestria, mark my fucking words.\n",
    "You think you can get away with saying that shit to me over the Ponynet?\n",
    "Think again, fucker.\n",
    "As we speak I am contacting my secret network of pegasi across Equestria and your hoofprints are being traced right now so you better prepare for the storm, maggot.\n",
    "The storm that wipes out the pathetic little thing you call your life. You’re fucking dead, pony. I can be anywhere, anytime, and I can hug you in over seven hundred ways, and that’s just with my bare hooves. Not only am I extensively trained in unarmed friendship, but I have access to the entire arsenal of ponies and I will use it to its full extent to wipe your miserable flank off the face of the continent, you little pony. If only you could have known what magical friendship your little “clever” comment was about to bring down upon you, maybe you would have held your fucking tongue. But you couldn’t, you didn’t, and now you’re paying the price, you goddamn pony. I will shit friendship all over you and you will drown in it. You’re fucking dead, pony.\n",
    "\"\"\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adsoj'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"adsoj   \".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "x = [random.random() < 1.0 for i in range(100000)]\n",
    "x = [y for y in x if y]\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_mask_from_lengths\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 5., 6., 2.], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True, False, False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,5,6,2]).cuda().float()\n",
    "print(x)\n",
    "mask = get_mask_from_lengths(x, max_len=int(x.max().item()))\n",
    "spec = torch.rand(4,160,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "print(   \"s\\r\\nt\".replace(\"\\r\\n\",\"\\n\")   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 1, 0, 9, 4, 6, 5, 3, 8, 2]\n",
      "[3, 1, 7, 2, 4, 5, 6, 9, 8, 0]\n",
      "[9, 1, 3, 0, 4, 6, 5, 2, 8, 7]\n",
      "[2, 1, 9, 7, 4, 5, 6, 0, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "arr = [0,1,2,3,4,5,6,7,8,9]\n",
    "arr = random.Random(1234).sample(arr, len(arr))\n",
    "print(arr)\n",
    "arr = random.Random(1234).sample(arr, len(arr))\n",
    "print(arr)\n",
    "arr = random.Random(1234).sample(arr, len(arr))\n",
    "print(arr)\n",
    "arr = random.Random(1234).sample(arr, len(arr))\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(-512//256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, invert this guy. Instead of splitting chunks of text that go over a limit. Split ALL the text at the start, and keep merging the text into chunks that are as large as possible!\n",
    "def parse_text_into_quotes(texts):\n",
    "    \"\"\"Swap speaker at every quote mark. Also split longer phrases at periods.\n",
    "    If text previously inside a quote is too long and split, then each split segment will have quotes around it (for information later on rather than accuracy to the original text).\"\"\"\n",
    "    max_text_segment_length = 120\n",
    "    quo ='\"' # nested quotes in list comprehension are hard to work with\n",
    "    texts = [f'\"{text.replace(quo,\"\").strip()}\"' if i%2 else text.replace(quo,\"\").strip() for i, text in enumerate(unidecode(texts).split('\"'))]\n",
    "    \n",
    "    texts_segmented = []\n",
    "    for text in texts:\n",
    "        text = text.strip()\n",
    "        if not len(text.replace('\"','').strip()): continue\n",
    "        text = text\\\n",
    "            .replace(\"\\n\",\" \")\\\n",
    "            .replace(\"  \",\" \")\\\n",
    "            .replace(\"> --------------------------------------------------------------------------\",\"\")\n",
    "        if len(text) > max_text_segment_length:\n",
    "            for seg in [x.strip() for x in sent_tokenize(text) if len(x.strip()) and x is not '\"']: \n",
    "                if '\"' in text:\n",
    "                    if seg[0] != '\"': seg='\"'+seg\n",
    "                    if seg[-1] != '\"': seg+='\"'\n",
    "                texts_segmented.append(seg)\n",
    "        else:\n",
    "            texts_segmented.append(text.strip())\n",
    "    return texts_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import nltk # sentence spliting\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator for text splitting.\n",
    "def parse_text_into_segments(texts, split_at_quotes=True, target_segment_length=200):\n",
    "    \"\"\"Swap speaker at every quote mark. Each split segment will have quotes around it (for information later on rather than accuracy to the original text).\"\"\"\n",
    "    \n",
    "    # split text by quotes\n",
    "    quo ='\"' # nested quotes in list comprehension are hard to work with\n",
    "    texts = [f'\"{text.replace(quo,\"\").strip()}\"' if i%2 else text.replace(quo,\"\").strip() for i, text in enumerate(unidecode(texts).split('\"'))]\n",
    "    \n",
    "    # clean up and remove empty texts\n",
    "    def clean_text(text):\n",
    "        text = text.strip()\n",
    "        text = text.replace(\"\\n\",\" \").replace(\"  \",\" \").replace(\"> --------------------------------------------------------------------------\",\"\")\n",
    "        return text\n",
    "    texts = [clean_text(text) for text in texts if len(text.strip().replace('\"','').strip()) or len(clean_text(text))]\n",
    "    assert len(texts)\n",
    "    \n",
    "    # split text by sentences and add commas in where needed.\n",
    "    def quotify(seg, text):\n",
    "        if '\"' in text:\n",
    "            if seg[0] != '\"': seg='\"'+seg\n",
    "            if seg[-1] != '\"': seg+='\"'\n",
    "        return seg\n",
    "    texts_tmp = []\n",
    "    texts = [texts_tmp.extend([quotify(x.strip(), text) for x in sent_tokenize(text) if len(x.strip()) and x is not '\"']) for text in texts]\n",
    "    texts = texts_tmp\n",
    "    del texts_tmp\n",
    "    assert len(texts)\n",
    "    \n",
    "    # merge neighbouring sentences\n",
    "    quote_mode = False\n",
    "    texts_output = []\n",
    "    texts_segmented = ''\n",
    "    texts_len = len(texts)\n",
    "    for i, text in enumerate(texts):\n",
    "        # split segment if quote swap\n",
    "        if split_at_quotes and ('\"' in text and quote_mode == False) or (not '\"' in text and quote_mode == True):\n",
    "            texts_segmented.replace('\"\"','')\n",
    "            texts_output.append(texts_segmented)\n",
    "            texts_segmented=text\n",
    "            quote_mode = not quote_mode\n",
    "        \n",
    "        # split segment if max length\n",
    "        elif len(texts_segmented+text) > target_segment_length:\n",
    "            texts_segmented.replace('\"\"','')\n",
    "            texts_output.append(texts_segmented)\n",
    "            texts_segmented=text\n",
    "        \n",
    "        else: # continue adding to segment\n",
    "            texts_segmented+= f' {text}'\n",
    "    # add any remaining stuff.\n",
    "    if len(texts_segmented):\n",
    "        texts_output.append(texts_segmented)\n",
    "    assert len(texts_output)\n",
    "    \n",
    "    return texts_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Helloo everypony!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = \"\"\"\n",
    "Helloo everypony!\n",
    "\"\"\"\n",
    "parse_text_into_segments(texts, split_at_quotes=True, target_segment_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 / 2 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802816"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "896*896*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0625"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(896*896*3) / (512*512*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.25"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(896*896*3) / (256*256*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cin × Cout × Lin + K × Cin × Lin\n",
    "K × Cin × Cout × Lin\n",
    "=\n",
    "1\n",
    "Cout\n",
    "+\n",
    "1\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443.40500673763256"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3**0.5) * 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786432"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(64,24,750).cuda()\n",
    "x *= 0\n",
    "\n",
    "print(x[x>=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5., device='cuda:0')\n",
      "tensor(5., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y = x\n",
    "x += 5\n",
    "print(x.mean())\n",
    "print(y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "299%300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.9 ns ± 1.01 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.8 µs ± 806 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# 4.1Ghz\n",
    "%%timeit\n",
    "_ = x.float().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 µs ± 661 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# 4.1Ghz\n",
    "%%timeit\n",
    "_ = x.exp() + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456 µs ± 16.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# 1.7Ghz\n",
    "%%timeit\n",
    "_ = x.exp() + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing some built in pytorch distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0923, 0.1653, 0.0973, 0.1973, 0.0498, 0.4577, 0.5144, 0.3580, 0.5851,\n",
      "        0.4594])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ -0.8330,  -2.1875,  -0.9353,  -2.7327,   0.1774,  -7.3737,  -8.5711,\n",
       "         -5.4807, -10.2455,  -7.4064])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "concentration1 = 0.1\n",
    "concentration0 = 10+0.9\n",
    "dist = torch.distributions.beta.Beta(concentration1, concentration0)\n",
    "value = torch.rand(10)\n",
    "print(value)\n",
    "dist.log_prob(value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0048)\n",
      "tensor(0.0989)\n",
      "tensor(0.1754)\n",
      "tensor(0.2879)\n",
      "tensor(0.3541)\n",
      "tensor(0.4220)\n",
      "tensor(0.5716)\n",
      "tensor(0.6179)\n",
      "tensor(0.7576)\n",
      "tensor(0.8267)\n",
      "tensor(0.9266)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "k = 0# index\n",
    "for k in range(11):\n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "    n = 10\n",
    "    concentration1 = k+a\n",
    "    concentration0 = n-k+b\n",
    "    dist = torch.distributions.beta.Beta(concentration1, concentration0)\n",
    "    print(  dist.sample((5,5)).mean()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4518)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.sample((5,5)).mean()\n",
    "# pi = log(P * ai-1)\n",
    "\n",
    "# on average,\n",
    "# energy[k] = (a*n)/(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a*n)/(a+b)\n",
    "# perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF overview of the DC Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_alignments: [batch, enc_T]\n",
    "\n",
    "# static convolution\n",
    "previous_alignments = tf.expand_dims(previous_alignments, axis=2) # [batch, enc_T] -> [batch, enc_T, 1]\n",
    "static_f = static_convolution(previous_alignments) # [batch, enc_T, 1] -> [batch, enc_T, attn_filters]\n",
    "static_f = static_fc(static_f)  # [batch, enc_T, attn_filters] -> [batch, enc_T, attn_dim]\n",
    "\n",
    "# dynamic convolution\n",
    "dynamic_filters = tf.layers.dense(tf.layers.dense(query, 128, activation=tf.tanh, use_bias=True, name=\"dynamic_fc1\"),\n",
    "                                  21 * 8, use_bias=False, name=\"dynamic_fc2\")\n",
    "dynamic_filters = tf.reshape(dynamic_filters, [-1, 21, 8])\n",
    "stacked_alignments = stack_alignments(previous_alignments)\n",
    "dynamic_f = tf.matmul(stacked_alignments, dynamic_filters)\n",
    "dynamic_f = dynamic_fc(dynamic_f) # [batch, enc_T, attn_dim]\n",
    "\n",
    "# score\n",
    "energy = compute_score(static_f, dynamic_f) # [batch, enc_T]\n",
    "\n",
    "# prior bias\n",
    "prior_filters = tf.convert_to_tensor(\n",
    "    [0.7400209, 0.07474979, 0.04157422, 0.02947039, 0.023170564, 0.019321883,\n",
    "     0.016758798, 0.014978543, 0.013751862, 0.013028075, 0.013172861], dtype=tf.float32)\n",
    "prior_filters = tf.reverse(prior_filters, axis=[0])\n",
    "prior_filters = tf.reshape(prior_filters, [11, 1, 1])\n",
    "\n",
    "bias = tf.nn.conv1d(tf.pad(previous_alignments, [[0, 0], [10, 0], [0, 0]]),\n",
    "                    prior_filters, stride=1, padding='VALID')\n",
    "bias = tf.maximum(tf.log(tf.squeeze(bias, axis=2)), -1.0e6)\n",
    "\n",
    "energy += bias\n",
    "alignments = _probability_fn(energy)  # softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and converted to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_alignments: [batch, enc_T]\n",
    "\n",
    "# static convolution\n",
    "previous_alignments = previous_alignments[..., None]\n",
    "static_f = self.location_layer(previous_alignments) # [B, 2, enc_T] -> [B, attention_n_filters, enc_T] -> [B, enc_T, attention_dim]\n",
    "\n",
    "# dynamic convolution\n",
    "dynamic_filters = torch.nn.Sequential(\n",
    "    nn.Linear(query, 128 , bias=True), # attention dim\n",
    "    nn.Tanh()\n",
    "    nn.Linear(128  , 21*8, bias=False) # filter_num * filter_length\n",
    "    )\n",
    "\n",
    "dynamic_filters = dynamic_filters.reshape([-1, 21, 8]) # [batch, filter_num, filter_length]\n",
    "stacked_alignments = previous_alignments.clone() #torch.stack(previous_alignments, dim=0) # which dim?\n",
    "dynamic_f = stacked_alignments @ dynamic_filters # [batch, enc_T] @ [batch, filter_num, filter_length] ->  [batch, enc_T, attn_dim]\n",
    "dynamic_f = self.vg(dynamic_f) # [batch, enc_T, attn_dim] -> [batch, enc_T, 1]\n",
    "\n",
    "# score # WTF does this do?\n",
    "energy = compute_score(static_f, dynamic_f) # [batch, enc_T]\n",
    "\n",
    "# prior bias\n",
    "prior_filters = torch.tensor(\n",
    "    [0.7400209, 0.07474979, 0.04157422, 0.02947039, 0.023170564, 0.019321883,\n",
    "     0.016758798, 0.014978543, 0.013751862, 0.013028075, 0.013172861])\n",
    "prior_filters = prior_filters.flip(dims=(0,))\n",
    "prior_filters = prior_filters[:, 1, 1]\n",
    "\n",
    "previous_alignments = F.pad(previous_alignments, [[0, 0], [10, 0], [0, 0]])\n",
    "bias = nn.conv1d(previous_alignments, prior_filters, stride=1, padding='VALID')\n",
    "bias = torch.clamp(bias.squeeze(2).log(), min=-1.0e6)\n",
    "\n",
    "energy += bias\n",
    "alignments = _probability_fn(energy)  # softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example Prior Filter code in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.01317286]]\n",
      "\n",
      " [[0.01302807]]\n",
      "\n",
      " [[0.01375186]]\n",
      "\n",
      " [[0.01497854]]\n",
      "\n",
      " [[0.0167588 ]]\n",
      "\n",
      " [[0.01932188]]\n",
      "\n",
      " [[0.02317056]]\n",
      "\n",
      " [[0.02947039]]\n",
      "\n",
      " [[0.04157422]]\n",
      "\n",
      " [[0.07474979]]\n",
      "\n",
      " [[0.7400209 ]]]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7400224 , 0.07474995, 0.04157431, 0.02947045, 0.02317061,\n",
       "        0.01932192, 0.01675883, 0.01497857, 0.01375189, 0.0130281 ,\n",
       "        0.01317289, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "prior_filters = tf.convert_to_tensor(\n",
    "    [0.7400209, 0.07474979, 0.04157422, 0.02947039, 0.023170564, 0.019321883,\n",
    "     0.016758798, 0.014978543, 0.013751862, 0.013028075, 0.013172861], dtype=tf.float32)\n",
    "# [11]\n",
    "\n",
    "prior_filters = tf.reverse(prior_filters, axis=[0]) # [11]\n",
    "prior_filters = tf.reshape(prior_filters, [11, 1, 1]) # [11, 1, 1]\n",
    "print( prior_filters.eval(session=tf.compat.v1.Session()) )\n",
    "\n",
    "prev_alignment = tf.one_hot([0], 60) # [1, 60] blank previous alignments\n",
    "\n",
    "print( prev_alignment.eval(session=tf.compat.v1.Session()) )\n",
    "\n",
    "for i in range(1):\n",
    "    expanded_alignment = tf.expand_dims(prev_alignment, axis=2) # [1, 60, 1]\n",
    "    expanded_alignment = tf.pad(expanded_alignment, [[0, 0], [10, 0], [0, 0]]) # [1, 70, 1]\n",
    "    energy = tf.nn.conv1d(expanded_alignment, prior_filters, stride=1, padding='VALID') # [1, 70, 1] -> [1, 60, 1] prior energy\n",
    "    energy = tf.squeeze(energy, axis=2) # [1, 60, 1] ->[1, 60]\n",
    "    energy = tf.log(energy) # [1, 60] -> [1, 60]\n",
    "    energy = tf.maximum(energy, -1.0e6)# [1, 60] -> [1, 60]\n",
    "    alignment = tf.nn.softmax(energy, axis=-1) # [1, 60]\n",
    "    prev_alignment = alignment # [1, 60]\n",
    "\n",
    "alignment.eval(session=tf.compat.v1.Session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and converted to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0132, 0.0130, 0.0138, 0.0150, 0.0168, 0.0193, 0.0232, 0.0295,\n",
      "          0.0416, 0.0747, 0.7400]]])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "tensor([[[0.7400, 0.0747, 0.0416, 0.0295, 0.0232, 0.0193, 0.0168, 0.0150,\n",
      "          0.0138, 0.0130, 0.0132, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "prior_filters = torch.tensor(\n",
    "    [0.7400209, 0.07474979, 0.04157422, 0.02947039, 0.023170564, 0.019321883,\n",
    "     0.016758798, 0.014978543, 0.013751862, 0.013028075, 0.013172861])\n",
    "# [11]\n",
    "\n",
    "prior_filters = prior_filters.flip(dims=(0,)) # [11]\n",
    "prior_filters = prior_filters[None, None, :] # [1, 1, 11]\n",
    "print(prior_filters)\n",
    "\n",
    "prev_alignment = torch.tensor([0.,]*60)[None, None, :] # [60] blank previous alignments\n",
    "prev_alignment[:,:,0] = 1.0\n",
    "print(prev_alignment)\n",
    "\n",
    "for i in range(1):\n",
    "    expanded_alignment = prev_alignment # [1, 60, 1]\n",
    "    expanded_alignment = F.pad(expanded_alignment, (10, 0)) # [1, 70, 1]\n",
    "    energy = F.conv1d(expanded_alignment, prior_filters) # [1, 70, 1] -> [1, 60, 1] prior energy\n",
    "    energy = (energy).log()\n",
    "    energy = torch.clamp(energy, min=-1.0e6)\n",
    "    alignment = F.softmax(energy, dim=-1) # [1, 60]\n",
    "    prev_alignment = alignment # [1, 60]\n",
    "\n",
    "print(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len ( [0.7400209, 0.07474979, 0.04157422, 0.02947039, 0.023170564, 0.019321883, 0.016758798, 0.014978543, 0.013751862, 0.013028075, 0.013172861] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Comparisons between Depthwise Convs and Normal Convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Conv MACs     = 7.077888B\n",
      "Seperable Conv MACs  = 3.987456B\n",
      "\n",
      "Normal Conv Params   = 35.38944M\n",
      "SeperableConv Params = 19.93728M\n",
      "\n",
      "Norn/Sep MACs = 1.78\n",
      "Norn/Sep Params = 1.78\n"
     ]
    }
   ],
   "source": [
    "segment_length = 24000\n",
    "\n",
    "norm_n_flows = 16\n",
    "norm_n_group = 120\n",
    "norm_n_layers = 5\n",
    "norm_n_channels = 384\n",
    "norm_kernel_width = 3\n",
    "\n",
    "sep_n_flows = 12\n",
    "sep_n_group = 120\n",
    "sep_n_layers = 4\n",
    "sep_n_channels = 640\n",
    "sep_kernel_width = 9\n",
    "####################################################################\n",
    "norm_mac = (norm_kernel_width*norm_n_channels**2) * (segment_length//norm_n_group)\n",
    "norm_par = (norm_kernel_width*norm_n_channels**2)\n",
    "sep_mac = ((sep_n_channels**2)+(sep_kernel_width*sep_n_channels)) * (segment_length//sep_n_group)\n",
    "sep_par = ((sep_n_channels**2)+(sep_kernel_width*sep_n_channels))\n",
    "###\n",
    "norm_mac *= norm_n_layers*norm_n_flows\n",
    "norm_par *= norm_n_layers*norm_n_flows\n",
    "sep_mac *= sep_n_layers*sep_n_flows\n",
    "sep_par *= sep_n_layers*sep_n_flows\n",
    "####################################################################\n",
    "print(\"Normal Conv MACs     = \", norm_mac/1e9, \"B\", sep=\"\")\n",
    "print(\"Seperable Conv MACs  = \", sep_mac/1e9, \"B\", sep=\"\")\n",
    "print(\"\")\n",
    "print(\"Normal Conv Params   = \", norm_par/1e6, \"M\", sep=\"\")\n",
    "print(\"SeperableConv Params = \", sep_par/1e6, \"M\", sep=\"\")\n",
    "print(\"\")\n",
    "print(\"Norn/Sep MACs =\", round(norm_mac/sep_mac, 2))\n",
    "print(\"Norn/Sep Params =\", round(norm_mac/sep_mac, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300//300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([5, 1])\n",
      "tensor([[0.4218, 0.9722, 0.3228, 0.6942, 0.3151],\n",
      "        [0.0851, 0.2514, 0.6336, 0.2688, 0.7350],\n",
      "        [0.2371, 0.7047, 0.6244, 0.6290, 0.8525],\n",
      "        [0.8339, 0.7792, 0.6584, 0.3497, 0.4097],\n",
      "        [0.8995, 0.4929, 0.1372, 0.6625, 0.3475]])\n",
      "tensor([[0.8814],\n",
      "        [0.9570],\n",
      "        [0.1764],\n",
      "        [0.0545],\n",
      "        [0.4619]])\n",
      "tensor([[0.3718, 0.8568, 0.2845, 0.6118, 0.2778],\n",
      "        [0.0814, 0.2405, 0.6063, 0.2572, 0.7034],\n",
      "        [0.0418, 0.1243, 0.1101, 0.1109, 0.1503],\n",
      "        [0.0454, 0.0424, 0.0359, 0.0190, 0.0223],\n",
      "        [0.4155, 0.2276, 0.0634, 0.3060, 0.1605]])\n",
      "tensor([[1.5424],\n",
      "        [0.7814],\n",
      "        [1.4215],\n",
      "        [1.8050],\n",
      "        [1.4852]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5,5)\n",
    "y = torch.rand(5,1)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x*y)\n",
    "print(x@y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get receptive field of each Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receptive Field per Flow = 7681\n",
      "Max Segment Length = 46080\n"
     ]
    }
   ],
   "source": [
    "n_flows = 12\n",
    "n_group = 120\n",
    "kernel_size = 9\n",
    "n_layers = 4\n",
    "r_field = ( n_group*(2**(n_layers-1))*(kernel_size-1) ) + 1\n",
    "print(f\"Receptive Field per Flow = {r_field}\")\n",
    "r_field_flows = (r_field//2)*n_flows\n",
    "print(f\"Max Segment Length = {r_field_flows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing some investigative state dicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint_path = r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing5\\best_val_model\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "state_dict = checkpoint['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker_embed.weight\n",
      "torch.Size([512, 96])\n",
      "\n",
      "cond_layers.0.bias\n",
      "torch.Size([256])\n",
      "\n",
      "cond_layers.0.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "cond_layers.0.weight_v\n",
      "torch.Size([256, 256, 3])\n",
      "\n",
      "cond_layers.1.bias\n",
      "torch.Size([256])\n",
      "\n",
      "cond_layers.1.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "cond_layers.1.weight_v\n",
      "torch.Size([256, 256, 3])\n",
      "\n",
      "cond_layers.2.bias\n",
      "torch.Size([160])\n",
      "\n",
      "cond_layers.2.weight_g\n",
      "torch.Size([160, 1, 1])\n",
      "\n",
      "cond_layers.2.weight_v\n",
      "torch.Size([160, 256, 1])\n",
      "\n",
      "convinv.0.weight\n",
      "torch.Size([24, 24, 1])\n",
      "\n",
      "convinv.1.weight\n",
      "torch.Size([24, 24, 1])\n",
      "\n",
      "convinv.2.weight\n",
      "torch.Size([24, 24, 1])\n",
      "\n",
      "convinv.3.weight\n",
      "torch.Size([24, 24, 1])\n",
      "\n",
      "convinv.4.weight\n",
      "torch.Size([24, 24, 1])\n",
      "\n",
      "convinv.5.weight\n",
      "torch.Size([24, 24, 1])\n",
      "\n",
      "WN.0.WN.in_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.in_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.in_layers.0.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.0.WN.in_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.in_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.in_layers.1.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.0.WN.in_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.in_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.in_layers.2.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.0.WN.in_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.in_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.in_layers.3.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.0.WN.in_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.in_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.in_layers.4.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.0.WN.in_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.in_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.in_layers.5.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.0.WN.in_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.in_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.in_layers.6.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.0.WN.in_layers.7.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.in_layers.7.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.in_layers.7.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.0.WN.res_skip_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.res_skip_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.0.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.res_skip_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.1.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.res_skip_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.2.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.res_skip_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.3.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.res_skip_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.4.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.res_skip_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.5.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.0.WN.res_skip_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.6.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.7.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.0.WN.res_skip_layers.7.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.0.WN.res_skip_layers.7.weight_v\n",
      "torch.Size([384, 384, 1])\n",
      "\n",
      "WN.0.WN.speaker_embed.weight\n",
      "torch.Size([512, 96])\n",
      "\n",
      "WN.0.WN.start.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.0.WN.start.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.0.WN.start.weight_v\n",
      "torch.Size([384, 12, 1])\n",
      "\n",
      "WN.0.WN.end.weight\n",
      "torch.Size([24, 384, 1])\n",
      "\n",
      "WN.0.WN.end.bias\n",
      "torch.Size([24])\n",
      "\n",
      "WN.0.WN.cond_layers.0.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.0.WN.cond_layers.0.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.0.WN.cond_layers.0.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.0.WN.cond_layers.1.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.0.WN.cond_layers.1.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.0.WN.cond_layers.1.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.0.WN.cond_layers.2.bias\n",
      "torch.Size([6144])\n",
      "\n",
      "WN.0.WN.cond_layers.2.weight_g\n",
      "torch.Size([6144, 1, 1])\n",
      "\n",
      "WN.0.WN.cond_layers.2.weight_v\n",
      "torch.Size([6144, 256, 1])\n",
      "\n",
      "WN.1.WN.in_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.in_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.in_layers.0.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.1.WN.in_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.in_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.in_layers.1.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.1.WN.in_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.in_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.in_layers.2.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.1.WN.in_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.in_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.in_layers.3.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.1.WN.in_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.in_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.in_layers.4.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.1.WN.in_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.in_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.in_layers.5.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.1.WN.in_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.in_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.in_layers.6.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.1.WN.in_layers.7.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.in_layers.7.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.in_layers.7.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.1.WN.res_skip_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.res_skip_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.0.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.res_skip_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.1.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.res_skip_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.2.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.res_skip_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.3.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.res_skip_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.4.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.res_skip_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.5.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.1.WN.res_skip_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.6.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.7.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.1.WN.res_skip_layers.7.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.1.WN.res_skip_layers.7.weight_v\n",
      "torch.Size([384, 384, 1])\n",
      "\n",
      "WN.1.WN.speaker_embed.weight\n",
      "torch.Size([512, 96])\n",
      "\n",
      "WN.1.WN.start.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.1.WN.start.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.1.WN.start.weight_v\n",
      "torch.Size([384, 12, 1])\n",
      "\n",
      "WN.1.WN.end.weight\n",
      "torch.Size([24, 384, 1])\n",
      "\n",
      "WN.1.WN.end.bias\n",
      "torch.Size([24])\n",
      "\n",
      "WN.1.WN.cond_layers.0.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.1.WN.cond_layers.0.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.1.WN.cond_layers.0.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.1.WN.cond_layers.1.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.1.WN.cond_layers.1.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.1.WN.cond_layers.1.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.1.WN.cond_layers.2.bias\n",
      "torch.Size([6144])\n",
      "\n",
      "WN.1.WN.cond_layers.2.weight_g\n",
      "torch.Size([6144, 1, 1])\n",
      "\n",
      "WN.1.WN.cond_layers.2.weight_v\n",
      "torch.Size([6144, 256, 1])\n",
      "\n",
      "WN.2.WN.in_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.in_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.in_layers.0.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.2.WN.in_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.in_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.in_layers.1.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.2.WN.in_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.in_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.in_layers.2.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.2.WN.in_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.in_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.in_layers.3.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.2.WN.in_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.in_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.in_layers.4.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.2.WN.in_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.in_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.in_layers.5.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.2.WN.in_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.in_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.in_layers.6.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.2.WN.in_layers.7.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.in_layers.7.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.in_layers.7.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.2.WN.res_skip_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.res_skip_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.0.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.res_skip_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.1.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.res_skip_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.2.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.res_skip_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.3.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.res_skip_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.4.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.res_skip_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.5.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.2.WN.res_skip_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.6.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.7.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.2.WN.res_skip_layers.7.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.2.WN.res_skip_layers.7.weight_v\n",
      "torch.Size([384, 384, 1])\n",
      "\n",
      "WN.2.WN.speaker_embed.weight\n",
      "torch.Size([512, 96])\n",
      "\n",
      "WN.2.WN.start.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.2.WN.start.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.2.WN.start.weight_v\n",
      "torch.Size([384, 12, 1])\n",
      "\n",
      "WN.2.WN.end.weight\n",
      "torch.Size([24, 384, 1])\n",
      "\n",
      "WN.2.WN.end.bias\n",
      "torch.Size([24])\n",
      "\n",
      "WN.2.WN.cond_layers.0.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.2.WN.cond_layers.0.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.2.WN.cond_layers.0.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.2.WN.cond_layers.1.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.2.WN.cond_layers.1.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.2.WN.cond_layers.1.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.2.WN.cond_layers.2.bias\n",
      "torch.Size([6144])\n",
      "\n",
      "WN.2.WN.cond_layers.2.weight_g\n",
      "torch.Size([6144, 1, 1])\n",
      "\n",
      "WN.2.WN.cond_layers.2.weight_v\n",
      "torch.Size([6144, 256, 1])\n",
      "\n",
      "WN.3.WN.in_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.in_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.in_layers.0.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.3.WN.in_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.in_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.in_layers.1.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.3.WN.in_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.in_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.in_layers.2.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.3.WN.in_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.in_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.in_layers.3.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.3.WN.in_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.in_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.in_layers.4.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.3.WN.in_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.in_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.in_layers.5.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.3.WN.in_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.in_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.in_layers.6.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.3.WN.in_layers.7.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.in_layers.7.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.in_layers.7.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.3.WN.res_skip_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.res_skip_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.0.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.res_skip_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.1.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.res_skip_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.2.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.res_skip_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.3.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.res_skip_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.4.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.res_skip_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.5.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.3.WN.res_skip_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.6.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.7.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.3.WN.res_skip_layers.7.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.3.WN.res_skip_layers.7.weight_v\n",
      "torch.Size([384, 384, 1])\n",
      "\n",
      "WN.3.WN.speaker_embed.weight\n",
      "torch.Size([512, 96])\n",
      "\n",
      "WN.3.WN.start.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.3.WN.start.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.3.WN.start.weight_v\n",
      "torch.Size([384, 12, 1])\n",
      "\n",
      "WN.3.WN.end.weight\n",
      "torch.Size([24, 384, 1])\n",
      "\n",
      "WN.3.WN.end.bias\n",
      "torch.Size([24])\n",
      "\n",
      "WN.3.WN.cond_layers.0.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.3.WN.cond_layers.0.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.3.WN.cond_layers.0.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.3.WN.cond_layers.1.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.3.WN.cond_layers.1.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.3.WN.cond_layers.1.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.3.WN.cond_layers.2.bias\n",
      "torch.Size([6144])\n",
      "\n",
      "WN.3.WN.cond_layers.2.weight_g\n",
      "torch.Size([6144, 1, 1])\n",
      "\n",
      "WN.3.WN.cond_layers.2.weight_v\n",
      "torch.Size([6144, 256, 1])\n",
      "\n",
      "WN.4.WN.in_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.in_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.in_layers.0.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.4.WN.in_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.in_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.in_layers.1.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.4.WN.in_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.in_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.in_layers.2.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.4.WN.in_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.in_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.in_layers.3.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.4.WN.in_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.in_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.in_layers.4.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.4.WN.in_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.in_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.in_layers.5.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.4.WN.in_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.in_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.in_layers.6.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.4.WN.in_layers.7.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.in_layers.7.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.in_layers.7.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.4.WN.res_skip_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.res_skip_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.0.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.res_skip_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.1.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.res_skip_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.2.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.res_skip_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.3.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.res_skip_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.4.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.res_skip_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.5.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.4.WN.res_skip_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.6.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.7.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.4.WN.res_skip_layers.7.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.4.WN.res_skip_layers.7.weight_v\n",
      "torch.Size([384, 384, 1])\n",
      "\n",
      "WN.4.WN.speaker_embed.weight\n",
      "torch.Size([512, 96])\n",
      "\n",
      "WN.4.WN.start.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.4.WN.start.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.4.WN.start.weight_v\n",
      "torch.Size([384, 12, 1])\n",
      "\n",
      "WN.4.WN.end.weight\n",
      "torch.Size([24, 384, 1])\n",
      "\n",
      "WN.4.WN.end.bias\n",
      "torch.Size([24])\n",
      "\n",
      "WN.4.WN.cond_layers.0.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.4.WN.cond_layers.0.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.4.WN.cond_layers.0.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.4.WN.cond_layers.1.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.4.WN.cond_layers.1.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.4.WN.cond_layers.1.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.4.WN.cond_layers.2.bias\n",
      "torch.Size([6144])\n",
      "\n",
      "WN.4.WN.cond_layers.2.weight_g\n",
      "torch.Size([6144, 1, 1])\n",
      "\n",
      "WN.4.WN.cond_layers.2.weight_v\n",
      "torch.Size([6144, 256, 1])\n",
      "\n",
      "WN.5.WN.in_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.in_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.in_layers.0.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.5.WN.in_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.in_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.in_layers.1.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.5.WN.in_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.in_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.in_layers.2.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.5.WN.in_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.in_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.in_layers.3.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.5.WN.in_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.in_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.in_layers.4.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.5.WN.in_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.in_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.in_layers.5.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.5.WN.in_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.in_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.in_layers.6.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.5.WN.in_layers.7.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.in_layers.7.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.in_layers.7.weight_v\n",
      "torch.Size([768, 384, 3])\n",
      "\n",
      "WN.5.WN.res_skip_layers.0.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.res_skip_layers.0.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.0.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.1.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.res_skip_layers.1.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.1.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.2.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.res_skip_layers.2.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.2.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.3.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.res_skip_layers.3.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.3.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.4.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.res_skip_layers.4.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.4.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.5.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.res_skip_layers.5.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.5.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.6.bias\n",
      "torch.Size([768])\n",
      "\n",
      "WN.5.WN.res_skip_layers.6.weight_g\n",
      "torch.Size([768, 1, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.6.weight_v\n",
      "torch.Size([768, 384, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.7.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.5.WN.res_skip_layers.7.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.5.WN.res_skip_layers.7.weight_v\n",
      "torch.Size([384, 384, 1])\n",
      "\n",
      "WN.5.WN.speaker_embed.weight\n",
      "torch.Size([512, 96])\n",
      "\n",
      "WN.5.WN.start.bias\n",
      "torch.Size([384])\n",
      "\n",
      "WN.5.WN.start.weight_g\n",
      "torch.Size([384, 1, 1])\n",
      "\n",
      "WN.5.WN.start.weight_v\n",
      "torch.Size([384, 12, 1])\n",
      "\n",
      "WN.5.WN.end.weight\n",
      "torch.Size([24, 384, 1])\n",
      "\n",
      "WN.5.WN.end.bias\n",
      "torch.Size([24])\n",
      "\n",
      "WN.5.WN.cond_layers.0.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.5.WN.cond_layers.0.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.5.WN.cond_layers.0.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.5.WN.cond_layers.1.bias\n",
      "torch.Size([256])\n",
      "\n",
      "WN.5.WN.cond_layers.1.weight_g\n",
      "torch.Size([256, 1, 1])\n",
      "\n",
      "WN.5.WN.cond_layers.1.weight_v\n",
      "torch.Size([256, 256, 1])\n",
      "\n",
      "WN.5.WN.cond_layers.2.bias\n",
      "torch.Size([6144])\n",
      "\n",
      "WN.5.WN.cond_layers.2.weight_g\n",
      "torch.Size([6144, 1, 1])\n",
      "\n",
      "WN.5.WN.cond_layers.2.weight_v\n",
      "torch.Size([6144, 256, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, weights in state_dict.items():\n",
    "    print(name)\n",
    "    print(weights.shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (End) Testing Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resave Checkpoint without optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... Writing to file... Done!\n",
      "\n",
      "Loading... Writing to file... Done!\n",
      "\n",
      "Loading... Writing to file... Done!\n",
      "\n",
      "G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_190000\n",
      "Doesn't exist. Skipping!\n",
      "\n",
      "G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_138000\n",
      "Doesn't exist. Skipping!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from os.path import exists\n",
    "\n",
    "checkpoint_paths = [\n",
    "    r\"G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_184000\",\n",
    "    r\"G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_186000\",\n",
    "    r\"G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_188000\",\n",
    "    r\"G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_190000\",\n",
    "    r\"G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_138000\",\n",
    "    ]\n",
    "\n",
    "for checkpoint_path in checkpoint_paths:\n",
    "    if not exists(checkpoint_path):\n",
    "        print(f\"{checkpoint_path}\\nDoesn't exist. Skipping!\\n\")\n",
    "        continue\n",
    "    \n",
    "    fpath = f\"{checkpoint_path}_weights\"\n",
    "    \n",
    "    if exists(fpath):\n",
    "        print(f\"{checkpoint_path}\\nAlready Exists! Skipping!\\n\")\n",
    "        continue\n",
    "    \n",
    "    print(\"Loading... \", end=\"\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    checkpoint['optimizer'] = None\n",
    "    \n",
    "    print(\"Writing to file... \", end=\"\")\n",
    "    torch.save(checkpoint, fpath)\n",
    "    del checkpoint\n",
    "    print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8192])\n",
      "tensor([[0.3208, 0.8105, 0.2217,  ..., 0.5874, 0.3625, 0.5889],\n",
      "        [0.3750, 0.8340, 0.4670,  ..., 0.1757, 0.3162, 0.8252],\n",
      "        [0.6411, 0.9746, 0.6665,  ..., 0.6582, 0.3889, 0.3081],\n",
      "        ...,\n",
      "        [0.5312, 0.9712, 0.6089,  ..., 0.0616, 0.7041, 0.2096],\n",
      "        [0.8179, 0.4597, 0.0719,  ..., 0.3877, 0.7627, 0.8511],\n",
      "        [0.1753, 0.4973, 0.7773,  ..., 0.9946, 0.8530, 0.4961]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   1,    1,    0,    1,    0,    0,    0,    2],\n",
       "        [   5,   21,    1,    1,    0,    1,   13,   10],\n",
       "        [ 997, 1432, 1857, 8191, 8191, 8191, 3277, 4891],\n",
       "        [ 997, 1432, 1857, 8191, 8191, 8191, 3277, 4891],\n",
       "        [8191, 8191, 8191, 8191, 8191, 8191, 8191, 8191]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(8,8192).cuda().half()\n",
    "#x *= ((torch.arange(8)[None, :].repeat(5,1)).float() / 4)\n",
    "\n",
    "def get_first_over_thresh(x, threshold=0.5):\n",
    "    x[:,x.size(1)-1] = threshold # set last to threshold just incase the output didn't finish generating.\n",
    "    x[x>threshold] = threshold\n",
    "    return x.size(1)-(x.flip(dims=(1,)).argmax(dim=1))\n",
    "\n",
    "def get_first_over_thresh(x, threshold):\n",
    "    \"\"\"Takes [B, T] and outputs first T over threshold for each B (output.shape = [B]).\"\"\"\n",
    "    x = x.cpu().float()\n",
    "    x[:,-1] = threshold # set last to threshold just incase the output didn't finish generating.\n",
    "    x[x>threshold] = threshold\n",
    "    return (x.size(1)-1)-(x.flip(dims=(1,)).argmax(dim=1))\n",
    "\n",
    "threshold = 0.9999\n",
    "\n",
    "print(x.shape)\n",
    "print(x)\n",
    "torch.stack((\n",
    "    get_first_over_thresh(x, 0.001),\n",
    "    get_first_over_thresh(x, 0.5),\n",
    "    get_first_over_thresh(x, 0.9),\n",
    "    get_first_over_thresh(x, 0.9999),\n",
    "    get_first_over_thresh(x, 1.0000),\n",
    "    get_first_over_thresh(x, 1.1)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(256, 8000).cuda()\n",
    "device = x.device\n",
    "print(device)\n",
    "x = x.cpu()\n",
    "print(x.device)\n",
    "x = x.to(device)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1458e-05, 1.9109e-04, 1.3828e-05,  ..., 2.8467e-04, 1.6344e-04,\n",
       "         4.4250e-04]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.load(\"gate_batch_outputs.npy\")\n",
    "torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4864'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{0.486421986412986412:0.4f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tacotron 2 inference code \n",
    "Edit the variables **checkpoint_path** and **text** to match yours and run the entire code to generate plots of mel outputs, alignments and audio synthesis from the generated mel-spectrogram using Griffin-Lim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and setup matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "D:\\Miniconda\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "import sys\n",
    "sys.path.append('waveglow/')\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hparams import create_hparams\n",
    "from model import Tacotron2\n",
    "from layers import TacotronSTFT, STFT\n",
    "from audio_processing import griffin_lim\n",
    "from train import load_model\n",
    "from text import text_to_sequence\n",
    "from denoiser import Denoiser\n",
    "from unidecode import unidecode\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running, Please wait...\n",
      "Dictionary Ready.\n"
     ]
    }
   ],
   "source": [
    "def plot_data(data, title=None):\n",
    "    %matplotlib inline\n",
    "    if len(data) > 1:\n",
    "        fig, axes = plt.subplots(len(data), 1, figsize=(int(alignment_graph_width*graph_scale/100), int(alignment_graph_height*graph_scale/100)))\n",
    "        axes = axes.flatten()\n",
    "        for i in range(len(data)):\n",
    "            if title:\n",
    "                axes[i].set_title(title[i])\n",
    "            axes[i].imshow(data[i], aspect='auto', origin='bottom', \n",
    "                           interpolation='none', cmap='inferno')\n",
    "        axes[0].set(xlabel=\"Frames\", ylabel=\"Channels\")\n",
    "        axes[1].set(xlabel=\"Decoder timestep\", ylabel=\"Encoder timestep\")\n",
    "    else:\n",
    "        fig, axes = plt.subplots(len(data), 1, figsize=(int(alignment_graph_width*graph_scale/100), int(alignment_graph_height*graph_scale/100)//2))\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        axes.imshow(data[0], aspect='auto', origin='bottom', interpolation='none', cmap='inferno')\n",
    "        axes.set(xlabel=\"Frames\", ylabel=\"Channels\")\n",
    "    fig.canvas.draw()\n",
    "    plt.show()\n",
    "\n",
    "#dictionary_path = r\"/media/cookie/Samsung PM961/TwiBot/tacotron2/filelists/merged.dict_.txt\"\n",
    "dictionary_path = r\"G:\\TwiBot\\tacotron2\\filelists\\merged.dict_.txt\"\n",
    "print(\"Running, Please wait...\")\n",
    "thisdict = {}\n",
    "for line in reversed((open(dictionary_path, \"r\").read()).splitlines()):\n",
    "    thisdict[(line.split(\" \", 1))[0]] = (line.split(\" \", 1))[1].strip()\n",
    "print(\"Dictionary Ready.\")\n",
    "sym = list(\"☺☻♥♦♣♠•◘○◙♂♀♪♫☼►◄↕‼¶§▬↨↑↓→←∟↔▲▼\")\n",
    "def ARPA(text_, punc=r\"!?,.;:␤#-_'\\\"()[]\"):\n",
    "    text = text_.replace(\"\\n\",\" \"); out = ''\n",
    "    for word_ in text.split(\" \"):\n",
    "        word=word_; end_chars = ''; start_chars = ''\n",
    "        while any(elem in word for elem in punc) and len(word) > 1:\n",
    "            if word[-1] in punc: end_chars = word[-1] + end_chars; word = word[:-1]\n",
    "            elif word[0] in punc: start_chars = start_chars + word[0]; word = word[1:]\n",
    "            else: break\n",
    "        try: word_arpa = thisdict[word.upper()]\n",
    "        except: word_arpa = ''\n",
    "        if len(word_arpa)!=0: word = \"{\" + str(word_arpa) + \"}\"\n",
    "        out = (out + \" \" + start_chars + word + end_chars).strip()\n",
    "    #if out[-1] != \"␤\": out = out + \"␤\"\n",
    "    #if out[0] != \"☺\": out = \"☺\" + out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "epochs=1000,iters_per_checkpoint=1000,iters_per_validation=1000,seed=1234,dynamic_loss_scaling=True,fp16_run=False,distributed_run=False,dist_backend=nccl,dist_url=tcp://127.0.0.1:54321,cudnn_enabled=True,cudnn_benchmark=False,ignore_layers=[],frozen_layers=[0],load_mel_from_disk=True,speakerlist=/media/cookie/Samsung 860 QVO/ClipperDatasetV2/filelists/speaker_ids.txt,training_files=/media/cookie/Samsung 860 QVO/ClipperDatasetV2/filelists/mel_train_taca2_merged.txt,validation_files=/media/cookie/Samsung 860 QVO/ClipperDatasetV2/filelists/mel_validation_taca2_merged.txt,text_cleaners=['basic_cleaners'],max_wav_value=32768.0,sampling_rate=48000,filter_length=2400,hop_length=600,win_length=2400,n_mel_channels=160,mel_fmin=0.0,mel_fmax=16000.0,n_symbols=179,symbols_embedding_dim=512,gate_positive_weight=10,gate_threshold=0.6,gate_delay=10,max_decoder_steps=1000,low_vram_inference=False,p_teacher_forcing=1.0,teacher_force_till=20,val_p_teacher_forcing=0.8,val_teacher_force_till=20,encoder_speaker_embed_dim=64,encoder_concat_speaker_embed=before_conv,encoder_kernel_size=5,encoder_n_convolutions=3,encoder_conv_hidden_dim=512,encoder_LSTM_dim=768,start_token=,stop_token=,hide_startstop_tokens=False,n_frames_per_step=1,context_frames=1,prenet_dim=512,prenet_layers=2,prenet_batchnorm=False,p_prenet_dropout=0.5,prenet_speaker_embed_dim=0,attention_rnn_dim=1280,AttRNN_extra_decoder_input=True,AttRNN_hidden_dropout_type=zoneout,p_AttRNN_hidden_dropout=0.1,p_AttRNN_cell_dropout=0.0,n_speakers=512,speaker_embedding_dim=256,decoder_rnn_dim=1792,extra_projection=False,DecRNN_hidden_dropout_type=zoneout,p_DecRNN_hidden_dropout=0.2,p_DecRNN_cell_dropout=0.0,attention_type=0,attention_dim=128,attention_location_n_filters=32,attention_location_kernel_size=31,num_att_mixtures=1,attention_layers=1,delta_offset=0,delta_min_limit=0,lin_bias=False,initial_gain=relu,normalize_attention_input=True,normalize_AttRNN_output=False,postnet_embedding_dim=512,postnet_kernel_size=5,postnet_n_convolutions=5,with_gst=True,ref_enc_pack_padded_seq=True,ref_enc_filters=[32, 32, 64, 64, 128, 128],ref_enc_size=[3, 3],ref_enc_strides=[2, 2],ref_enc_pad=[1, 1],ref_enc_gru_size=128,gstAtt_dim=128,num_heads=8,token_num=5,token_activation_func=tanh,token_embedding_size=256,torchMoji_attDim=2304,torchMoji_linear=True,torchMoji_training=True,p_drop_tokens=0.0,drop_tokens_mode=zeros,use_saved_learning_rate=False,loss_func=MSELoss,learning_rate=1e-06,weight_decay=1e-06,grad_clip_thresh=1.0,batch_size=28,val_batch_size=28,truncated_length=640,mask_padding=True,global_mean_npy=global_mean.npy,drop_frame_rate=0.25\n"
     ]
    }
   ],
   "source": [
    "hparams = create_hparams()\n",
    "hparams.max_decoder_steps = 1000\n",
    "hparams.gate_threshold = 0.6\n",
    "hparams.ignore_layers = []\n",
    "print(str(hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows WaveGlow from Ground Truth\n",
    "from utils import load_wav_to_torch\n",
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate, max_value = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / max(max_value, audio.max(), -audio.min())\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Tacotron2 model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tacotron... Done\n",
      "This Tacotron model has been trained for  53000  Iterations.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = r\"G:\\TwiBot\\tacotron2-PPP-1.3.0\\outdir_truncated1\\checkpoint_53000\"\n",
    "print(\"Loading Tacotron... \", end=\"\")\n",
    "checkpoint_hparams = torch.load(checkpoint_path)['hparams']\n",
    "#checkpoint_hparams.parse_json(hparams.to_json())\n",
    "model = load_model(checkpoint_hparams)\n",
    "#checkpoint_dict = {k.replace(\"encoder_speaker_embedding.weight\",\"encoder.encoder_speaker_embedding.weight\"): v for k,v in torch.load(checkpoint_path)['state_dict'].items()}\n",
    "checkpoint_dict = torch.load(checkpoint_path)['state_dict']\n",
    "model.load_state_dict(checkpoint_dict)\n",
    "_ = model.cuda().eval().half(); print(\"Done\")\n",
    "\n",
    "tacotron_speaker_id_lookup = torch.load(checkpoint_path)['speaker_id_lookup']\n",
    "print(\"This Tacotron model has been trained for \",torch.load(checkpoint_path)['iteration'],\" Iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decoder.prenet.layers.0.linear_layer.weight',\n",
       " 'decoder.prenet.layers.1.linear_layer.weight',\n",
       " 'decoder.attention_rnn.weight_ih',\n",
       " 'decoder.attention_rnn.weight_hh',\n",
       " 'decoder.attention_rnn.bias_ih',\n",
       " 'decoder.attention_rnn.bias_hh',\n",
       " 'decoder.attention_layer.query_layer.linear_layer.weight',\n",
       " 'decoder.attention_layer.memory_layer.linear_layer.weight',\n",
       " 'decoder.attention_layer.v.linear_layer.weight',\n",
       " 'decoder.attention_layer.location_layer.location_conv.conv.weight',\n",
       " 'decoder.attention_layer.location_layer.location_dense.linear_layer.weight',\n",
       " 'decoder.decoder_rnn.weight_ih',\n",
       " 'decoder.decoder_rnn.weight_hh',\n",
       " 'decoder.decoder_rnn.bias_ih',\n",
       " 'decoder.decoder_rnn.bias_hh',\n",
       " 'decoder.linear_projection.linear_layer.weight',\n",
       " 'decoder.linear_projection.linear_layer.bias',\n",
       " 'decoder.gate_layer.linear_layer.weight',\n",
       " 'decoder.gate_layer.linear_layer.bias']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in list(model.named_parameters()) if x[0].startswith(\"decoder\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load WaveGlow from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yoyo': True, 'yoyo_WN': False, 'n_mel_channels': 160, 'n_flows': 6, 'n_group': 24, 'n_early_every': 6, 'n_early_size': 2, 'memory_efficient': False, 'spect_scaling': False, 'upsample_mode': 'normal', 'WN_config': {'n_layers': 8, 'n_channels': 384, 'kernel_size': 3, 'speaker_embed_dim': 96, 'rezero': False}, 'win_length': 2400, 'hop_length': 600}\n",
      "Config File from 'H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing5\\config.json' successfully loaded.\n",
      "intializing WaveGlow model... Done!\n",
      "loading WaveGlow checkpoint... Done!\n",
      "initializing Denoiser... Done!\n",
      "359500 iterations\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "waveglow_path = r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing5\\best_val_model\" #r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing2\\best_val_model\"\n",
    "config_fpath =  r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing5\\config.json\" #r\"H:\\TTCheckpoints\\waveglow\\outdir_EfficientSmallGlobalSpeakerEmbeddings\\Testing2\\config.json\"\n",
    "\n",
    "# Load config file\n",
    "with open(config_fpath) as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)\n",
    "train_config = config[\"train_config\"]\n",
    "global data_config\n",
    "data_config = config[\"data_config\"]\n",
    "global dist_config\n",
    "dist_config = config[\"dist_config\"]\n",
    "global waveglow_config\n",
    "waveglow_config = {\n",
    "    **config[\"waveglow_config\"], \n",
    "    'win_length': data_config['win_length'],\n",
    "    'hop_length': data_config['hop_length']\n",
    "}\n",
    "print(waveglow_config)\n",
    "print(f\"Config File from '{config_fpath}' successfully loaded.\")\n",
    "\n",
    "# import the correct model\n",
    "if waveglow_config[\"yoyo\"]: # efficient_mode # TODO: Add to Config File\n",
    "    from efficient_model import WaveGlow\n",
    "else:\n",
    "    from glow import WaveGlow\n",
    "\n",
    "# initialize model\n",
    "print(f\"intializing WaveGlow model... \", end=\"\")\n",
    "waveglow = WaveGlow(**waveglow_config).cuda()\n",
    "print(f\"Done!\")\n",
    "\n",
    "# load checkpoint from file\n",
    "print(f\"loading WaveGlow checkpoint... \", end=\"\")\n",
    "checkpoint = torch.load(waveglow_path)\n",
    "waveglow.load_state_dict(checkpoint['model']) # and overwrite initialized weights with checkpointed weights\n",
    "waveglow.cuda().eval().half() # move to GPU and convert to half precision\n",
    "print(f\"Done!\")\n",
    "#for k in waveglow.convinv:\n",
    "#    k.float()\n",
    "print(f\"initializing Denoiser... \", end=\"\")\n",
    "denoiser = Denoiser(waveglow)\n",
    "print(f\"Done!\")\n",
    "waveglow_iters = torch.load(waveglow_path)['iteration']\n",
    "print(waveglow_iters, \"iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resave Checkpoint without optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "checkpoint['optimizer'] = None\n",
    "\n",
    "fpath = waveglow_path.replace(r\"\\best_val_model\",r\"\\best_val_weights\")\n",
    "if  not exists(fpath):\n",
    "    torch.save(checkpoint, fpath)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Already Exists! Skipping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x = [0,1,2,3,4,5,6,7]\n",
    "for i in range(20):\n",
    "    print(x[i%len(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_names = ['one','two','three']\n",
    "def shuffle_and_return():\n",
    "    speaker_names.append(speaker_names.pop(0))\n",
    "    return speaker_names[0]\n",
    "simultaneous_texts = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'three']\n"
     ]
    }
   ],
   "source": [
    "batch_speaker_names = [shuffle_and_return() for i in range(simultaneous_texts)]\n",
    "print(batch_speaker_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 1) Get Speaker ID's from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tacotron_speaker_id_lookup\n",
    "waveglow_speaker_id_lookup = checkpoint['speaker_lookup']\n",
    "print(str(waveglow_speaker_id_lookup).replace(\",\",\"\\n\").replace(\":\",\" ->\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 2) Rebuild Speaker ID's from training filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_utils import TextMelLoader\n",
    "#from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#speaker_ids = TextMelLoader(\"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/filelists/mel_train_taca2.txt\", hparams).speaker_ids\n",
    "#speaker_ids = TextMelLoader(r\"D:\\ClipperDatasetV2/filelists/mel_train_taca2.txt\", hparams, check_files=False, TBPTT=False).speaker_ids\n",
    "#print(str(speaker_ids).replace(\", \",\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load TorchMoji for Style Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\" Use torchMoji to score texts for emoji distribution.\n",
    "\n",
    "The resulting emoji ids (0-63) correspond to the mapping\n",
    "in emoji_overview.png file at the root of the torchMoji repo.\n",
    "\n",
    "Writes the result to a csv file.\n",
    "\"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "\n",
    "maxlen = 180\n",
    "texts = [\"Testing!\",]\n",
    "\n",
    "with torch.no_grad():\n",
    "    st = SentenceTokenizer(vocabulary, maxlen, ignore_sentences_with_only_custom=True)\n",
    "    torchmoji = torchmoji_feature_encoding(PRETRAINED_PATH)\n",
    "    tokenized, _, _ = st.tokenize_sentences(texts) # input array [B] e.g: [\"Test?\",\"2nd Sentence!\"]\n",
    "    embedding = torchmoji(tokenized) # returns np array [B, Embed]\n",
    "    print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a measure for Alignment quality in inferred clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths, max_len=None):\n",
    "    if not max_len:\n",
    "        max_len = torch.max(lengths).long()\n",
    "    ids = torch.arange(0, max_len, device=lengths.device, dtype=torch.int64)\n",
    "    mask = (ids < lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "# New MUCH more performant version, (doesn't support unique padded inputs, just iterate over the batch dim or smthn if you need padded inputs cause this is still way faster)\n",
    "#@torch.jit.script # should work and be even faster, but makes it harder to debug and it's already fast enough right now\n",
    "def alignment_metric(alignments, input_lengths=None, output_lengths=None, average_across_batch=False):\n",
    "    alignments = alignments.transpose(1,2) # [B, dec, enc] -> [B, enc, dec]\n",
    "    # alignments [batch size, x, y]\n",
    "    # input_lengths [batch size] for len_x\n",
    "    # output_lengths [batch size] for len_y\n",
    "    if input_lengths == None:\n",
    "        input_lengths =  torch.ones(alignments.size(0), device=alignments.device)*(alignments.shape[1]-1) # [B] # 147\n",
    "    if output_lengths == None:\n",
    "        output_lengths = torch.ones(alignments.size(0), device=alignments.device)*(alignments.shape[2]-1) # [B] # 767\n",
    "    batch_size = alignments.size(0)\n",
    "    optimums = torch.sqrt(input_lengths.double().pow(2) + output_lengths.double().pow(2)).view(batch_size)\n",
    "    \n",
    "    # [B, enc, dec] -> [B, dec], [B, dec]\n",
    "    values, cur_idxs = torch.max(alignments, 1) # get max value in column and location of max value\n",
    "    \n",
    "    cur_idxs = cur_idxs.float()\n",
    "    prev_indx = torch.cat((cur_idxs[:,0][:,None], cur_idxs[:,:-1]), dim=1) # shift entire tensor by one.\n",
    "    dist = ((prev_indx - cur_idxs).pow(2) + 1).pow(0.5) # [B, dec]\n",
    "    dist.masked_fill_(~get_mask_from_lengths(output_lengths, max_len=dist.size(1)), 0.0) # set dist of padded to zero\n",
    "    dist = dist.sum(dim=(1)) # get total dist for each B\n",
    "    diagonalitys = (dist + 1.4142135)/optimums # dist / optimal dist\n",
    "    \n",
    "    alignments.masked_fill_(~get_mask_from_lengths(output_lengths, max_len=alignments.size(2))[:,None,:], 0.0)\n",
    "    att_enc_total = torch.sum(alignments, dim=2)# [B, enc, dec] -> [B, enc]\n",
    "    \n",
    "    # calc max (with padding ignored)\n",
    "    att_enc_total.masked_fill_(~get_mask_from_lengths(input_lengths, max_len=att_enc_total.size(1)), 0.0)\n",
    "    encoder_max_focus = att_enc_total.max(dim=1)[0] # [B, enc] -> [B]\n",
    "    \n",
    "    # calc mean (with padding ignored)\n",
    "    encoder_avg_focus = att_enc_total.mean(dim=1)   # [B, enc] -> [B]\n",
    "    encoder_avg_focus *= (att_enc_total.size(1)/input_lengths.float())\n",
    "    \n",
    "    # calc min (with padding ignored)\n",
    "    att_enc_total.masked_fill_(~get_mask_from_lengths(input_lengths, max_len=att_enc_total.size(1)), 1.0)\n",
    "    encoder_min_focus = att_enc_total.min(dim=1)[0] # [B, enc] -> [B]\n",
    "    \n",
    "    # calc average max attention (with padding ignored)\n",
    "    values.masked_fill_(~get_mask_from_lengths(output_lengths, max_len=values.size(1)), 0.0) # because padding\n",
    "    avg_prob = values.mean(dim=1)\n",
    "    avg_prob *= (alignments.size(2)/output_lengths.float()) # because padding\n",
    "    \n",
    "    if average_across_batch:\n",
    "        diagonalitys = diagonalitys.mean()\n",
    "        encoder_max_focus = encoder_max_focus.mean()\n",
    "        encoder_min_focus = encoder_min_focus.mean()\n",
    "        encoder_avg_focus = encoder_avg_focus.mean()\n",
    "        avg_prob = avg_prob.mean()\n",
    "    return diagonalitys.cpu(), avg_prob.cpu(), encoder_max_focus.cpu(), encoder_min_focus.cpu(), encoder_avg_focus.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesize audio (From Filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from time import sleep\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\"\"\"\n",
    "|(Game) Them's Fightin' Herds_Oleander|150\n",
    "|(Game) Them's Fightin' Herds_Fred|151\n",
    "|(Game) Them's Fightin' Herds_Pom|152\n",
    "|(Game) Them's Fightin' Herds_Velvet|153\n",
    "|(Game) Them's Fightin' Herds_Arizona|154\n",
    "|(Game) Them's Fightin' Herds_Tianhuo|155\n",
    "|(Game) Elite Dangerous_Eli|156\n",
    "|(Audiobook) A Little Bit Wicked_Skystar|157\n",
    "|(Audiobook) Dr. Who_Doctor|158\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "|(Show) My Little Pony_Rainbow|160\n",
    "|(Show) My Little Pony_Pinkie|161\n",
    "|(Show) My Little Pony_Rarity|162\n",
    "|(Show) My Little Pony_Spike|163\n",
    "|(Show) My Little Pony_Fluttershy|164\n",
    "|(Show) My Little Pony_Nightmare Moon|165\n",
    "|(Show) Dan Vs_Dan|166\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "|(Show) My Little Pony_Scootaloo|168\n",
    "|(Show) My Little Pony_Big Mac|169\n",
    "|(Show) My Little Pony_Sweetie Belle|170\n",
    "|(Show) My Little Pony_Apple Bloom|171\n",
    "\"\"\"\n",
    "\n",
    "speakers = \"\"\"\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "narrators = \"\"\"\n",
    "|(Audiodrama) Fallout Equestria_Littlepip|1\n",
    "|(Game) Them's Fightin' Herds_Oleander|150\n",
    "|(Game) Them's Fightin' Herds_Pom|152\n",
    "|(Game) Them's Fightin' Herds_Velvet|153\n",
    "|(Game) Them's Fightin' Herds_Arizona|154\n",
    "|(Game) Them's Fightin' Herds_Tianhuo|155\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "|(Show) My Little Pony_Rainbow|160\n",
    "|(Show) My Little Pony_Pinkie|161\n",
    "|(Show) My Little Pony_Rarity|162\n",
    "|(Show) My Little Pony_Spike|163\n",
    "|(Show) My Little Pony_Fluttershy|164\n",
    "|(Show) Dan Vs_Dan|166\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "|(Show) My Little Pony_Scootaloo|168\n",
    "|(Show) My Little Pony_Sweetie Belle|170\n",
    "|(Show) My Little Pony_Apple Bloom|171\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "texts = \"\"\"\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmm, that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmm, that feels nice.\n",
    "Mmmmmmmmmmmmmmm, that feels nice.\n",
    "Mmmmmmmmmmmm that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm that feels nice.\n",
    "Mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm that feels nice.\n",
    "\"\"\"\n",
    "_audio_path_override = None\n",
    "_speaker_id_override = None\n",
    "style_mode = 'torchmoji_hidden' # Options = 'mel','token','zeros','torchmoji_hidden','torchmoji_string'\n",
    "\n",
    "acceptable_score = 0.8 # sufficient score to just skip ahead instead of checking/generating more outputs\n",
    "absolutely_required_score = 0.3 # retry forever until this score is reached\n",
    "absolute_maximum_tries = 256 # this is per text text input\n",
    "\n",
    "# Score Parameters\n",
    "diagonality_weighting = 0.5 # 'stutter factor', a penalty for clips where the model jumps back and forwards in the sentence.\n",
    "max_focus_weighting = 1.0   # 'stuck factor', a penalty for clips that spend execisve time on the same letter.\n",
    "min_focus_weighting = 1.0   # 'miniskip factor', a penalty for skipping/ignoring small parts of the input text.\n",
    "avg_focus_weighting = 1.0   # 'skip factor', a penalty for skipping very large parts of the input text\n",
    "max_attempts = 128 # retries at each clip # this is per text input\n",
    "batch_size_per_text = 128 # minibatch_size per unique text input\n",
    "simultaneous_texts = 1 # num unique text inputs per batch\n",
    "\n",
    "max_decoder_steps = 1600\n",
    "max_text_segment_length = 120\n",
    "gate_threshold = 0.7\n",
    "gate_delay = 3\n",
    "use_arpabet = 1\n",
    "\n",
    "sigma = 0.95\n",
    "audio_save_path = r\"D:\\Downloads\\infer\\testing\"\n",
    "output_filename = 'TestingOutput'\n",
    "save_wavs = 1 # saves wavs to infer folder\n",
    "\n",
    "show_all_attempt_scores = 0\n",
    "show_audio_overwrite_warnings = 1\n",
    "show_input_text = 1\n",
    "show_best_score = 1\n",
    "show_audio  = 1\n",
    "show_graphs_tacotron = 1\n",
    "show_graphs_waveglow = 1\n",
    "status_updates = 1 # ... Done\n",
    "time_to_gen = 1\n",
    "graph_scale = 0.5\n",
    "alignment_graph_width = 3840\n",
    "alignment_graph_height = 1920\n",
    "\n",
    "model.decoder.gate_delay = gate_delay\n",
    "model.decoder.max_decoder_steps = max_decoder_steps\n",
    "model.decoder.gate_threshold = gate_threshold\n",
    "\n",
    "os.makedirs(audio_save_path, exist_ok=True)\n",
    "texts_segmented = [x for x in texts.split(\"\\n\") if len(x.strip())]\n",
    "total_len = len(texts_segmented)\n",
    "\n",
    "continue_from = 0 # skip\n",
    "counter = 0\n",
    "text_batch_in_progress = []\n",
    "for text_index, text in enumerate(texts_segmented):\n",
    "    if text_index < continue_from: print(f\"Skipping {text_index}.\\t\",end=\"\"); counter+=1; continue\n",
    "    print(f\"{text_index}/{total_len}|{datetime.now()}\")\n",
    "    \n",
    "    # setup the text batches\n",
    "    text_batch_in_progress.append(text)\n",
    "    if (len(text_batch_in_progress) == simultaneous_texts) or (text_index == (len(texts_segmented)-1)): # if text batch ready or final input\n",
    "        text_batch = text_batch_in_progress\n",
    "        text_batch_in_progress = []\n",
    "        if (text_index == (len(texts_segmented)-1)): # if final text input\n",
    "            simultaneous_texts = len(text_batch) # ensure batch size is still correct\n",
    "    else:\n",
    "        continue # if batch not ready, add another text\n",
    "    \n",
    "    # pick the speakers for the texts\n",
    "    speaker_ids = [random.choice(speakers).split(\"|\")[2] if ('\"' in text) else random.choice(narrators).split(\"|\")[2] for text in text_batch] # pick speaker if quotemark in text, else narrator\n",
    "    text_batch  = [text.replace('\"',\"\") for text in text_batch] # remove quotes from text\n",
    "    \n",
    "    if _audio_path_override != None:\n",
    "        audio_path = _audio_path_override\n",
    "    if _speaker_id_override != None:\n",
    "        speaker_id = _speaker_id_override\n",
    "    \n",
    "    # get speaker_ids (tacotron)\n",
    "    tacotron_speaker_ids = [tacotron_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    tacotron_speaker_ids = torch.LongTensor(tacotron_speaker_ids).cuda().repeat_interleave(batch_size_per_text)\n",
    "    \n",
    "    # get speaker_ids (waveglow)\n",
    "    waveglow_speaker_ids = [waveglow_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    waveglow_speaker_ids = torch.LongTensor(waveglow_speaker_ids).cuda()\n",
    "    \n",
    "    # style\n",
    "    if style_mode == 'mel':\n",
    "        mel = load_mel(audio_path.replace(\".npy\",\".wav\")).cuda().half()\n",
    "        style_input = mel\n",
    "    elif style_mode == 'token':\n",
    "        pass\n",
    "        #style_input =\n",
    "    elif style_mode == 'zeros':\n",
    "        style_input = None\n",
    "    elif style_mode == 'torchmoji_hidden':\n",
    "        try:\n",
    "            tokenized, _, _ = st.tokenize_sentences(text_batch) # input array [B] e.g: [\"Test?\",\"2nd Sentence!\"]\n",
    "        except:\n",
    "            raise Exception(f\"text\\n{text_batch}\\nfailed to tokenize.\")\n",
    "        try:\n",
    "            embedding = torchmoji(tokenized) # returns np array [B, Embed]\n",
    "        except Exception as ex:\n",
    "            print(f'Exception: {ex}')\n",
    "            print(f\"text: {text_batch} failed to process.\")\n",
    "            #raise Exception(f\"text\\n{text}\\nfailed to process.\")\n",
    "        style_input = torch.from_numpy(embedding).cuda().half().repeat_interleave(batch_size_per_text, dim=0)\n",
    "    elif style_mode == 'torchmoji_string':\n",
    "        style_input = text_batch\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # check punctuation\n",
    "    valid_last_char = '-,.?!;:' # valid final characters in texts\n",
    "    text_batch = [text+'.' if (text[-1] not in ',.?!;:') else text for text in text_batch]\n",
    "    \n",
    "    # parse text\n",
    "    text_batch = [unidecode(text.replace(\"...\",\". \").replace(\"  \",\" \").strip()) for text in text_batch] # remove eclipses, double spaces, unicode and spaces before/after the text.\n",
    "    if show_input_text: # debug\n",
    "        print(\"raw_text:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    if use_arpabet: # convert texts to ARPAbet (phonetic) versions.\n",
    "        text_batch = [ARPA(text) for text in text_batch]\n",
    "    if show_input_text: # debug\n",
    "        print(\"model_input:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if time_to_gen:\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # convert texts to sequence, pad where appropriate and move to GPU\n",
    "        sequence_split = [torch.LongTensor(text_to_sequence(text, ['english_cleaners'])) for text in text_batch] # convert texts to numpy representation\n",
    "        text_lengths = torch.tensor([seq.size(0) for seq in sequence_split])\n",
    "        max_len = text_lengths.max().item()\n",
    "        sequence = torch.zeros(text_lengths.size(0), max_len).long() # create large tensor to move each text input into\n",
    "        for i in range(text_lengths.size(0)): # move each text into padded input tensor\n",
    "            sequence[i, :sequence_split[i].size(0)] = sequence_split[i]\n",
    "        sequence = sequence.cuda().long().repeat_interleave(batch_size_per_text, dim=0) # move to GPU and repeat text\n",
    "        text_lengths = text_lengths.cuda().long() # move to GPU\n",
    "        #print(\"max_len =\", max_len) # debug\n",
    "        #print( get_mask_from_lengths(text_lengths).shape ) # debug\n",
    "        #sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long().repeat_interleave(batch_size_per_text, 0)# convert numpy to tensor and repeat for each text\n",
    "        \n",
    "        # debug\n",
    "        text_lengths = text_lengths.clone()\n",
    "        sequence = sequence.clone()\n",
    "        \n",
    "        for i in range(1):\n",
    "            try:\n",
    "                best_score = np.ones(simultaneous_texts) * -9e9\n",
    "                tries      = np.zeros(simultaneous_texts)\n",
    "                best_generations = [0]*simultaneous_texts\n",
    "                best_score_str = ['']*simultaneous_texts\n",
    "                while np.amin(best_score) < acceptable_score:\n",
    "                    # run inference\n",
    "                    if status_updates: print(\"Running Tacotron2... \", end='')\n",
    "                    mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = model.inference(sequence, tacotron_speaker_ids, style_input=style_input, style_mode=style_mode, text_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0))\n",
    "                    \n",
    "                    # find metrics for each item\n",
    "                    gate_batch_outputs[:,:20] = 0 # ignore gate predictions for the first bit\n",
    "                    output_lengths = gate_batch_outputs.argmax(dim=1)+gate_delay\n",
    "                    diagonality_batch, avg_prob_batch, enc_max_focus_batch, enc_min_focus_batch, enc_avg_focus_batch = alignment_metric(alignments_batch, input_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0), output_lengths=output_lengths)\n",
    "                    \n",
    "                    batch = list(zip(\n",
    "                        mel_batch_outputs.split(1,dim=0),\n",
    "                        mel_batch_outputs_postnet.split(1,dim=0),\n",
    "                        gate_batch_outputs.split(1,dim=0),\n",
    "                        alignments_batch.split(1,dim=0),\n",
    "                        diagonality_batch,\n",
    "                        avg_prob_batch,\n",
    "                        enc_max_focus_batch,\n",
    "                        enc_min_focus_batch,\n",
    "                        enc_avg_focus_batch,))\n",
    "                    # split batch into items\n",
    "                    \n",
    "                    for j in range(simultaneous_texts): # process each set of text spectrograms seperately\n",
    "                        start, end = (j*batch_size_per_text), ((j+1)*batch_size_per_text)\n",
    "                        sametext_batch = batch[start:end] # seperate the full batch into pieces that use the same input text\n",
    "                        \n",
    "                        # process all items related to the j'th text input\n",
    "                        for k, (mel_outputs, mel_outputs_postnet, gate_outputs, alignments, diagonality, avg_prob, enc_max_focus, enc_min_focus, enc_avg_focus) in enumerate(sametext_batch):\n",
    "                            # factors that make up score\n",
    "                            weighted_score =  avg_prob.item() # general alignment quality\n",
    "                            weighted_score -= (max(diagonality.item(),1.11)-1.11) * diagonality_weighting  # consistent pace\n",
    "                            weighted_score -= max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting # getting stuck on pauses/phones\n",
    "                            weighted_score -= max(0.9-enc_min_focus.item(),0) * min_focus_weighting # skipping single enc outputs\n",
    "                            weighted_score -= max(2.5-enc_avg_focus.item(), 0) * avg_focus_weighting # skipping most enc outputs\n",
    "                            score_str = f\"{round(diagonality.item(),3)} {round(avg_prob.item()*100,2)}% {round(weighted_score,4)} {round(max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting,2)} {round(max(0.9-enc_min_focus.item(),0),2)}|\"\n",
    "                            if weighted_score > best_score[j]:\n",
    "                                best_score[j] = weighted_score\n",
    "                                best_score_str[j] = score_str\n",
    "                                best_generations[j] = [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "                            if show_all_attempt_scores:\n",
    "                                print(score_str, end=\"\")\n",
    "                            tries[j]+=1\n",
    "                            if np.amin(tries) >= max_attempts and np.amin(best_score) > (absolutely_required_score-1):\n",
    "                                raise StopIteration\n",
    "                            if np.amin(tries) >= absolute_maximum_tries:\n",
    "                                print(f\"Absolutely required score not achieved in {absolute_maximum_tries} attempts - \", end='')\n",
    "                                raise StopIteration\n",
    "                    \n",
    "                    if np.amin(tries) < (max_attempts-1):\n",
    "                        print('Acceptable alignment/diagonality not reached. Retrying.')\n",
    "                    elif np.amin(best_score) < absolutely_required_score:\n",
    "                        print('Score less than absolutely required score. Retrying extra.')\n",
    "            except StopIteration:\n",
    "                del batch\n",
    "                if status_updates: print(\"Done\")\n",
    "                pass\n",
    "            # [[mel, melpost, gate, align], [mel, melpost, gate, align], [mel, melpost, gate, align]] -> [[mel, mel, mel], [melpost, melpost, melpost], [gate, gate, gate], [align, align, align]]\n",
    "            \n",
    "            # zip is being weird so alternative used\n",
    "            mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = [x[0][0].T for x in best_generations], [x[1][0].T for x in best_generations], [x[2][0] for x in best_generations], [x[3][0] for x in best_generations] # pickup whatever was the best attempts\n",
    "            \n",
    "            # stack arrays into tensors\n",
    "            gate_batch_outputs = torch.nn.utils.rnn.pad_sequence(gate_batch_outputs, batch_first=True, padding_value=0)\n",
    "#            print(gate_batch_outputs.shape)\n",
    "            max_length = torch.max(gate_batch_outputs.argmax(dim=1)) # get highest duration\n",
    "            mel_batch_outputs = torch.nn.utils.rnn.pad_sequence(mel_batch_outputs, batch_first=True, padding_value=-11.6).transpose(1,2)[:,:,:max_length]\n",
    "            mel_batch_outputs_postnet = torch.nn.utils.rnn.pad_sequence(mel_batch_outputs_postnet, batch_first=True, padding_value=-11.6).transpose(1,2)[:,:,:max_length]\n",
    "            alignments_batch = torch.nn.utils.rnn.pad_sequence(alignments_batch, batch_first=True, padding_value=0)[:,:max_length,:]\n",
    "            \n",
    "            if status_updates: print(\"Running WaveGlow... \", end='')\n",
    "            audio_batch = waveglow.infer(mel_batch_outputs_postnet, speaker_ids=waveglow_speaker_ids, sigma=sigma)\n",
    "            audio_denoised_batch = denoiser(audio_batch, strength=0.0001)[:, 0]\n",
    "            if status_updates: print('Done')\n",
    "            \n",
    "            audio_len = 0\n",
    "            for j, (audio, audio_denoised) in enumerate(zip(audio_batch.split(1, dim=0), audio_denoised_batch.split(1, dim=0))):\n",
    "                # remove WaveGlow padding\n",
    "                audio_end = (gate_batch_outputs[j].argmax()+gate_delay) * hparams.hop_length\n",
    "                audio = audio[:,:audio_end]\n",
    "                audio_denoised = audio_denoised[:,:audio_end]\n",
    "                \n",
    "                # remove Tacotron2 padding\n",
    "                spec_end = gate_batch_outputs[j].argmax()+gate_delay\n",
    "                mel_outputs = mel_batch_outputs.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"mel_outputs.split(blah)[j].shape\", mel_outputs.shape)\n",
    "                mel_outputs_postnet = mel_batch_outputs_postnet.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"alignments_batch.shape\", alignments_batch.shape)\n",
    "                alignments = alignments_batch.split(1, dim=0)[j][:,:spec_end,:text_lengths[j]]\n",
    "                ##print(\"alignments.split(blah)[j].shape\", alignments.shape)\n",
    "                \n",
    "                if show_best_score:\n",
    "                    print(f\"Score: {round(best_score[j],3)}\\t\\tStats: {best_score_str[j]}  Verified: {[x.item() for x in alignment_metric(alignments)]}\")\n",
    "                if show_graphs_tacotron:\n",
    "                    plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
    "                           alignments.float().data.cpu().numpy()[0].T), title=[\"Spectrogram (Tacotron)\",\"Alignment (Tacotron)\"])\n",
    "                if show_audio:\n",
    "                        ipd.display(ipd.Audio(audio.cpu().numpy(), rate=hparams.sampling_rate))        \n",
    "                if save_wavs:\n",
    "                    save_audio_path = os.path.join(audio_save_path,f\"audio_{counter//300:02}_{counter:05}.wav\")\n",
    "                    if os.path.exists(save_audio_path):\n",
    "                        if show_audio_overwrite_warnings:\n",
    "                            print(f\"File already found at [{save_audio_path}], overwriting.\")\n",
    "                        os.remove(save_audio_path)\n",
    "                    if status_updates: print(f\"Saving clip to [{save_audio_path}]... \", end=\"\")\n",
    "                    librosa.output.write_wav(save_audio_path, np.swapaxes(audio.float().cpu().numpy(),0,1), hparams.sampling_rate)\n",
    "                    if status_updates: print(\"Done\")\n",
    "                if show_graphs_waveglow and save_wavs:\n",
    "                    plot_data([load_mel(save_audio_path).float().data.cpu().numpy()[0],], title = \"Spectrogram (WaveGlow)\")\n",
    "                counter+=1\n",
    "                audio_len+=audio_end\n",
    "            \n",
    "            if time_to_gen:\n",
    "                audio_seconds_generated = round(audio_len.item()/hparams.sampling_rate,3)\n",
    "                print(f\"Took {round(time.time()-start_time,3)}s to generate {audio_seconds_generated}s of audio. (best of {tries.sum().astype('int')} tries)\")\n",
    "            \n",
    "            print(\"\")\n",
    "_text = None; _audio_path = None; _speaker_id = None\n",
    "\n",
    "# Merge clips and output the concatenated result\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# get number of intermediate concatenations required (Sox can only merge 340~ files at a time)\n",
    "n_audio_batches = -(-len( glob(os.path.join(audio_save_path, \"audio_*_*.wav\")) ) // 300)\n",
    "\n",
    "# ensure path ends in .wav\n",
    "if not output_filename[-4:].lower() == '.wav':\n",
    "    output_filename+='.wav'\n",
    "\n",
    "for i in range(n_audio_batches):\n",
    "    print(f\"Merging audio files {i*300} to {((i+1)*300)-1}... \", end='')\n",
    "    os.system(f'sox {os.path.join(audio_save_path, f\"audio_{i:02}_*.wav\")} -b 16 {os.path.join(audio_save_path, f\"concat_{i:02}.wav\")}')\n",
    "    print(\"Done\")\n",
    "print(f\"Saving output to '{os.path.join(audio_save_path, output_filename)}'... \", end='')\n",
    "os.system(f'sox \"{os.path.join(audio_save_path, \"concat_*.wav\")}\" -b 16 \"{os.path.join(audio_save_path, output_filename)}\"') # merge the merged files into a final output. bit depth of 16 required to go over 4 hour length\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "x = [1,2,3]\n",
    "[i for i in x for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64*3*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "48000/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_users = [\"Twilight Sparkle\",\"Rainbow Dash\",\"Fluttershy\",\"Applejack\"]\n",
    "difflib.get_close_matches(\"TS\", valid_users, n=4, cutoff=0.01)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(0).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1,2,3,4,5,6,7][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthesize audio (From Text List e.g Fimfic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    # list(chunks([0,1,2,3,4,5,6,7,8,9],2)) -> [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def parse_txt_into_quotes(fpath):\n",
    "    texts = open(fpath, \"r\", encoding=\"utf-8\").read()\n",
    "    \n",
    "    quo ='\"'; texts = [f'\"{text.replace(quo,\"\").strip()}\"' if i%2 else text.replace(quo,\"\").strip() for i, text in enumerate(unidecode(texts).split('\"'))]\n",
    "    \n",
    "    texts_segmented = []\n",
    "    for text in texts:\n",
    "        text = text.strip()\n",
    "        if not len(text.replace('\"','').strip()): continue\n",
    "        text = text\\\n",
    "            .replace(\"\\n\",\" \")\\\n",
    "            .replace(\"  \",\" \")\\\n",
    "            .replace(\"> --------------------------------------------------------------------------\",\"\")\n",
    "        if len(text) > max_text_segment_length:\n",
    "            for seg in [x.strip() for x in text.split(\".\") if len(x.strip()) if x is not '\"']: \n",
    "                if '\"' in text:\n",
    "                    if seg[0] != '\"': seg='\"'+seg\n",
    "                    if seg[-1] != '\"': seg+='\"'\n",
    "                texts_segmented.append(seg)\n",
    "        else:\n",
    "            texts_segmented.append(text.strip())\n",
    "    return texts_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from time import sleep\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\"\"\"\n",
    "|(Game) Them's Fightin' Herds_Oleander|150\n",
    "|(Game) Them's Fightin' Herds_Fred|151\n",
    "|(Game) Them's Fightin' Herds_Pom|152\n",
    "|(Game) Them's Fightin' Herds_Velvet|153\n",
    "|(Game) Them's Fightin' Herds_Arizona|154\n",
    "|(Game) Them's Fightin' Herds_Tianhuo|155\n",
    "|(Game) Elite Dangerous_Eli|156\n",
    "|(Audiobook) A Little Bit Wicked_Skystar|157\n",
    "|(Audiobook) Dr. Who_Doctor|158\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "|(Show) My Little Pony_Rainbow|160\n",
    "|(Show) My Little Pony_Pinkie|161\n",
    "|(Show) My Little Pony_Rarity|162\n",
    "|(Show) My Little Pony_Spike|163\n",
    "|(Show) My Little Pony_Fluttershy|164\n",
    "|(Show) My Little Pony_Nightmare Moon|165\n",
    "|(Show) Dan Vs_Dan|166\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "|(Show) My Little Pony_Scootaloo|168\n",
    "|(Show) My Little Pony_Big Mac|169\n",
    "|(Show) My Little Pony_Sweetie Belle|170\n",
    "|(Show) My Little Pony_Apple Bloom|171\n",
    "\"\"\"\n",
    "\n",
    "speakers = \"\"\"\n",
    "|(Show) My Little Pony_Twilight|167\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "narrators = \"\"\"\n",
    "|(Show) My Little Pony_Applejack|159\n",
    "\"\"\".replace(\"_\",\", \").replace(\"(\",\" \").replace(\")\",\", \").split(\"\\n\")[1:-1]\n",
    "\n",
    "_audio_path_override = None\n",
    "_speaker_id_override = None\n",
    "style_mode = 'torchmoji_hidden' # Options = 'mel','token','zeros','torchmoji_hidden','torchmoji_string'\n",
    "\n",
    "acceptable_score = 0.8 # sufficient score to just skip ahead instead of checking/generating more outputs\n",
    "absolutely_required_score = 0.3 # retry forever until this score is reached\n",
    "absolute_maximum_tries = 512 # this is per text text input\n",
    "\n",
    "# Score Parameters\n",
    "diagonality_weighting = 0.5 # 'stutter factor', a penalty for clips where the model jumps back and forwards in the sentence.\n",
    "max_focus_weighting = 1.0   # 'stuck factor', a penalty for clips that spend execisve time on the same letter.\n",
    "min_focus_weighting = 1.0   # 'miniskip factor', a penalty for skipping/ignoring small parts of the input text.\n",
    "avg_focus_weighting = 1.0   # 'skip factor', a penalty for skipping very large parts of the input text\n",
    "max_attempts = 256 # retries at each clip # this is per text input\n",
    "batch_size_per_text = 256 # minibatch_size per unique text input\n",
    "simultaneous_texts = 1 # num unique text inputs per batch\n",
    "\n",
    "max_decoder_steps = 1600\n",
    "max_text_segment_length = 120\n",
    "gate_threshold = 0.7\n",
    "gate_delay = 3\n",
    "use_arpabet = 1\n",
    "\n",
    "sigma = 0.95\n",
    "audio_save_path = r\"D:\\Downloads\\infer\\audio\"\n",
    "output_filename = 'Mort Takes a Holiday'\n",
    "save_wavs = 1 # saves wavs to infer folder\n",
    "\n",
    "show_all_attempt_scores = 0\n",
    "show_audio_overwrite_warnings = 1\n",
    "show_input_text = 1\n",
    "show_best_score = 1\n",
    "show_audio  = 1\n",
    "show_graphs = 1\n",
    "status_updates = 1 # ... Done\n",
    "time_to_gen = 1\n",
    "graph_scale = 0.5\n",
    "alignment_graph_width = 3840\n",
    "alignment_graph_height = 1920\n",
    "\n",
    "model.decoder.gate_delay = gate_delay\n",
    "model.decoder.max_decoder_steps = max_decoder_steps\n",
    "model.decoder.gate_threshold = gate_threshold\n",
    "\n",
    "file_path = r\"D:\\Downloads\\infer\\text\\Mort Takes a Holiday.txt\"\n",
    "\n",
    "texts_segmented = parse_txt_into_quotes(file_path)\n",
    "\n",
    "total_len = len(texts_segmented)\n",
    "\n",
    "# 0 init\n",
    "# 1 append\n",
    "# 2 append, generate, blank\n",
    "# 1 append\n",
    "# 2 append, generate, blank\n",
    "# 1\n",
    "# 2\n",
    "\n",
    "continue_from = 2545 # skip\n",
    "counter = 0\n",
    "text_batch_in_progress = []\n",
    "for text_index, text in enumerate(texts_segmented):\n",
    "    if text_index < continue_from: print(f\"Skipping {text_index}.\\t\",end=\"\"); counter+=1; continue\n",
    "    print(f\"{text_index}/{total_len}|{datetime.now()}\")\n",
    "    \n",
    "    # setup the text batches\n",
    "    text_batch_in_progress.append(text)\n",
    "    if (len(text_batch_in_progress) == simultaneous_texts) or (text_index == (len(texts_segmented)-1)): # if text batch ready or final input\n",
    "        text_batch = text_batch_in_progress\n",
    "        text_batch_in_progress = []\n",
    "    else:\n",
    "        continue # if batch not ready, add another text\n",
    "    \n",
    "    # pick the speakers for the texts\n",
    "    speaker_ids = [random.choice(speakers).split(\"|\")[2] if ('\"' in text) else random.choice(narrators).split(\"|\")[2] for text in text_batch] # pick speaker if quotemark in text, else narrator\n",
    "    text_batch  = [text.replace('\"',\"\") for text in text_batch] # remove quotes from text\n",
    "    \n",
    "    if _audio_path_override != None:\n",
    "        audio_path = _audio_path_override\n",
    "    if _speaker_id_override != None:\n",
    "        speaker_id = _speaker_id_override\n",
    "    \n",
    "    # get speaker_ids (tacotron)\n",
    "    tacotron_speaker_ids = [tacotron_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    tacotron_speaker_ids = torch.LongTensor(tacotron_speaker_ids).cuda().repeat_interleave(batch_size_per_text)\n",
    "    \n",
    "    # get speaker_ids (waveglow)\n",
    "    waveglow_speaker_ids = [waveglow_speaker_id_lookup[int(speaker_id)] for speaker_id in speaker_ids]\n",
    "    waveglow_speaker_ids = torch.LongTensor(waveglow_speaker_ids).cuda()\n",
    "    \n",
    "    # style\n",
    "    if style_mode == 'mel':\n",
    "        mel = load_mel(audio_path.replace(\".npy\",\".wav\")).cuda().half()\n",
    "        style_input = mel\n",
    "    elif style_mode == 'token':\n",
    "        pass\n",
    "        #style_input =\n",
    "    elif style_mode == 'zeros':\n",
    "        style_input = None\n",
    "    elif style_mode == 'torchmoji_hidden':\n",
    "        try:\n",
    "            tokenized, _, _ = st.tokenize_sentences(text_batch) # input array [B] e.g: [\"Test?\",\"2nd Sentence!\"]\n",
    "        except:\n",
    "            raise Exception(f\"text\\n{text_batch}\\nfailed to tokenize.\")\n",
    "        try:\n",
    "            embedding = torchmoji(tokenized) # returns np array [B, Embed]\n",
    "        except Exception as ex:\n",
    "            print(f'Exception: {ex}')\n",
    "            print(f\"text: '{text_batch}' failed to process.\")\n",
    "            #raise Exception(f\"text\\n{text}\\nfailed to process.\")\n",
    "        style_input = torch.from_numpy(embedding).cuda().half().repeat_interleave(batch_size_per_text, dim=0)\n",
    "    elif style_mode == 'torchmoji_string':\n",
    "        style_input = text_batch\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # check punctuation\n",
    "    valid_last_char = '-,.?!;:' # valid final characters in texts\n",
    "    text_batch = [text+'.' if (text[-1] not in ',.?!;:') else text for text in text_batch]\n",
    "    \n",
    "    # parse text\n",
    "    text_batch = [unidecode(text.replace(\"...\",\". \").replace(\"  \",\" \").strip()) for text in text_batch] # remove eclipses, double spaces, unicode and spaces before/after the text.\n",
    "    if show_input_text: # debug\n",
    "        print(\"raw_text:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    if use_arpabet: # convert texts to ARPAbet (phonetic) versions.\n",
    "        text_batch = [ARPA(text) for text in text_batch]\n",
    "    if show_input_text: # debug\n",
    "        print(\"model_input:\\n\", \"\\n\".join([str(j)+': \\''+text+'\\'' for j, text in enumerate(text_batch)]), sep='')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if time_to_gen:\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # convert texts to sequence, pad where appropriate and move to GPU\n",
    "        sequence_split = [torch.LongTensor(text_to_sequence(text, ['english_cleaners'])) for text in text_batch] # convert texts to numpy representation\n",
    "        text_lengths = torch.tensor([seq.size(0) for seq in sequence_split])\n",
    "        max_len = text_lengths.max().item()\n",
    "        sequence = torch.zeros(text_lengths.size(0), max_len).long() # create large tensor to move each text input into\n",
    "        for i in range(text_lengths.size(0)): # move each text into padded input tensor\n",
    "            sequence[i, :sequence_split[i].size(0)] = sequence_split[i]\n",
    "        sequence = sequence.cuda().long().repeat_interleave(batch_size_per_text, dim=0) # move to GPU and repeat text\n",
    "        text_lengths = text_lengths.cuda().long() # move to GPU\n",
    "        #print(\"max_len =\", max_len) # debug\n",
    "        #print( get_mask_from_lengths(text_lengths).shape ) # debug\n",
    "        #sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long().repeat_interleave(batch_size_per_text, 0)# convert numpy to tensor and repeat for each text\n",
    "        \n",
    "        # debug\n",
    "        text_lengths = text_lengths.clone()\n",
    "        sequence = sequence.clone()\n",
    "        \n",
    "        for i in range(1):\n",
    "            try:\n",
    "                best_score = np.ones(simultaneous_texts) * -9e9\n",
    "                tries      = np.zeros(simultaneous_texts)\n",
    "                best_generations = [0]*simultaneous_texts\n",
    "                best_score_str = ['']*simultaneous_texts\n",
    "                while np.amin(best_score) < acceptable_score:\n",
    "                    # run inference\n",
    "                    if status_updates: print(\"Running Tacotron2... \", end='')\n",
    "                    mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = model.inference(sequence, tacotron_speaker_ids, style_input=style_input, style_mode=style_mode, text_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0))\n",
    "                    \n",
    "                    # find metrics for each item\n",
    "                    gate_batch_outputs[:,:20] = 0 # ignore gate predictions for the first 0.05s\n",
    "                    output_lengths = gate_batch_outputs.argmax(dim=1)+gate_delay\n",
    "                    diagonality_batch, avg_prob_batch, enc_max_focus_batch, enc_min_focus_batch, enc_avg_focus_batch = alignment_metric(alignments_batch, input_lengths=text_lengths.repeat_interleave(batch_size_per_text, dim=0), output_lengths=output_lengths)\n",
    "                    \n",
    "                    batch = list(zip(\n",
    "                        mel_batch_outputs.split(1,dim=0),\n",
    "                        mel_batch_outputs_postnet.split(1,dim=0),\n",
    "                        gate_batch_outputs.split(1,dim=0),\n",
    "                        alignments_batch.split(1,dim=0),\n",
    "                        diagonality_batch,\n",
    "                        avg_prob_batch,\n",
    "                        enc_max_focus_batch,\n",
    "                        enc_min_focus_batch,\n",
    "                        enc_avg_focus_batch,))\n",
    "                    # split batch into items\n",
    "                    \n",
    "                    for j in range(simultaneous_texts): # process each set of text spectrograms seperately\n",
    "                        start, end = (j*batch_size_per_text), ((j+1)*batch_size_per_text)\n",
    "                        sametext_batch = batch[start:end] # seperate the full batch into pieces that use the same input text\n",
    "                        \n",
    "                        # process all items related to the j'th text input\n",
    "                        for k, (mel_outputs, mel_outputs_postnet, gate_outputs, alignments, diagonality, avg_prob, enc_max_focus, enc_min_focus, enc_avg_focus) in enumerate(sametext_batch):\n",
    "                            # factors that make up score\n",
    "                            weighted_score =  avg_prob.item() # general alignment quality\n",
    "                            weighted_score -= (max(diagonality.item(),1.11)-1.11) * diagonality_weighting  # consistent pace\n",
    "                            weighted_score -= max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting # getting stuck on pauses/phones\n",
    "                            weighted_score -= max(0.9-enc_min_focus.item(),0) * min_focus_weighting # skipping single enc outputs\n",
    "                            weighted_score -= max(2.5-enc_avg_focus.item(), 0) * avg_focus_weighting # skipping most enc outputs\n",
    "                            score_str = f\"{round(diagonality.item(),3)} {round(avg_prob.item()*100,2)}% {round(weighted_score,4)} {round(max((enc_max_focus.item()-20), 0) * 0.005 * max_focus_weighting,2)} {round(max(0.9-enc_min_focus.item(),0),2)}|\"\n",
    "                            if weighted_score > best_score[j]:\n",
    "                                best_score[j] = weighted_score\n",
    "                                best_score_str[j] = score_str\n",
    "                                best_generations[j] = [mel_outputs, mel_outputs_postnet, gate_outputs, alignments]\n",
    "                            if show_all_attempt_scores:\n",
    "                                print(score_str, end=\"\")\n",
    "                            tries[j]+=1\n",
    "                            if np.amin(tries) >= max_attempts and np.amin(best_score) > (absolutely_required_score-1):\n",
    "                                raise StopIteration\n",
    "                            if np.amin(tries) >= absolute_maximum_tries:\n",
    "                                print(f\"Absolutely required score not achieved in {absolute_maximum_tries} attempts - \", end='')\n",
    "                                raise StopIteration\n",
    "                    \n",
    "                    if np.amin(tries) < (max_attempts-1):\n",
    "                        print('Acceptable alignment/diagonality not reached. Retrying.')\n",
    "                    elif np.amin(best_score) < absolutely_required_score:\n",
    "                        print('Score less than absolutely required score. Retrying extra.')\n",
    "            except StopIteration:\n",
    "                del batch\n",
    "                if status_updates: print(\"Done\")\n",
    "                pass\n",
    "            # [[mel, melpost, gate, align], [mel, melpost, gate, align], [mel, melpost, gate, align]] -> [[mel, mel, mel], [melpost, melpost, melpost], [gate, gate, gate], [align, align, align]]\n",
    "            \n",
    "            # zip is being weird so alternative used\n",
    "            mel_batch_outputs, mel_batch_outputs_postnet, gate_batch_outputs, alignments_batch = [x[0] for x in best_generations], [x[1] for x in best_generations], [x[2] for x in best_generations], [x[3] for x in best_generations] # pickup whatever was the best attempts\n",
    "            \n",
    "            # stack arrays into tensors\n",
    "            gate_batch_outputs = torch.cat(gate_batch_outputs, dim=0)\n",
    "            max_length = torch.max(gate_batch_outputs.argmax(dim=1)) # get highest duration\n",
    "            mel_batch_outputs = torch.cat(mel_batch_outputs, dim=0)[:,:,:max_length]\n",
    "            mel_batch_outputs_postnet = torch.cat(mel_batch_outputs_postnet, dim=0)[:,:,:max_length]\n",
    "            alignments_batch = torch.cat(alignments_batch, dim=0)[:,:max_length,:]\n",
    "            ##print(\"max_length =\", max_length)\n",
    "            ##print(\"gate_batch_outputs.argmax(dim=1) =\", gate_batch_outputs.argmax(dim=1))\n",
    "            ##print(\"mel_batch_outputs.shape\", mel_batch_outputs.shape)\n",
    "            ##print(\"mel_batch_outputs_postnet.shape\", mel_batch_outputs_postnet.shape)\n",
    "            ##print(\"alignments_batch.shape\", alignments_batch.shape)\n",
    "            \n",
    "            if status_updates: print(\"Running WaveGlow... \", end='')\n",
    "            audio_batch = waveglow.infer(mel_batch_outputs_postnet, speaker_ids=waveglow_speaker_ids, sigma=sigma)\n",
    "            audio_denoised_batch = denoiser(audio_batch, strength=0.0001)[:, 0]\n",
    "            if status_updates: print('Done')\n",
    "            \n",
    "            audio_len = 0\n",
    "            for j, (audio, audio_denoised) in enumerate(zip(audio_batch.split(1, dim=0), audio_denoised_batch.split(1, dim=0))):\n",
    "                # remove WaveGlow padding\n",
    "                audio_end = (gate_batch_outputs[j].argmax()+gate_delay) * hparams.hop_length\n",
    "                audio = audio[:,:audio_end]\n",
    "                audio_denoised = audio_denoised[:,:audio_end]\n",
    "                \n",
    "                # remove Tacotron2 padding\n",
    "                spec_end = gate_batch_outputs[j].argmax()+gate_delay\n",
    "                mel_outputs = mel_batch_outputs.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"mel_outputs.split(blah)[j].shape\", mel_outputs.shape)\n",
    "                mel_outputs_postnet = mel_batch_outputs_postnet.split(1, dim=0)[j][:,:,:spec_end]\n",
    "                ##print(\"alignments_batch.shape\", alignments_batch.shape)\n",
    "                alignments = alignments_batch.split(1, dim=0)[j][:,:spec_end,:text_lengths[j]]\n",
    "                ##print(\"alignments.split(blah)[j].shape\", alignments.shape)\n",
    "                \n",
    "                if show_best_score:\n",
    "                    print(f\"Score: {round(best_score[j],3)}\\t\\tStats: {best_score_str[j]}  Verified: {[x.item() for x in alignment_metric(alignments)]}\")\n",
    "                if show_graphs:\n",
    "                    plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
    "                           alignments.float().data.cpu().numpy()[0].T))\n",
    "                if show_audio:\n",
    "                        ipd.display(ipd.Audio(audio.cpu().numpy(), rate=hparams.sampling_rate))        \n",
    "                if save_wavs:\n",
    "                    save_audio_path = os.path.join(audio_save_path,f\"audio_{counter//300:02}_{counter:05}.wav\")\n",
    "                    if os.path.exists(save_audio_path):\n",
    "                        if show_audio_overwrite_warnings:\n",
    "                            print(f\"File already found at [{save_audio_path}], overwriting.\")\n",
    "                        os.remove(save_audio_path)\n",
    "                    if status_updates: print(f\"Saving clip to [{save_audio_path}]... \", end=\"\")\n",
    "                    librosa.output.write_wav(save_audio_path, np.swapaxes(audio.float().cpu().numpy(),0,1), hparams.sampling_rate)\n",
    "                    if status_updates: print(\"Done\")\n",
    "                counter+=1\n",
    "                audio_len+=audio_end\n",
    "            \n",
    "            if time_to_gen:\n",
    "                audio_seconds_generated = round(audio_len.item()/hparams.sampling_rate,3)\n",
    "                print(f\"Took {round(time.time()-start_time,3)}s to generate {audio_seconds_generated}s of audio. (best of {tries.sum().astype('int')} tries)\")\n",
    "                #print(\"spec_end/max_len = \", spec_end/max_len) # debug\n",
    "                #print(\"spec_end/max_len = \", spec_end/max_len) # debug\n",
    "            \n",
    "            print(\"\")\n",
    "_text = None; _audio_path = None; _speaker_id = None\n",
    "\n",
    "# Merge clips and output the concatenated result\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# get number of intermediate concatenations required (Sox and only merge 340~ files at a time)\n",
    "n_audio_batches = round(len( glob(os.path.join(audio_save_path, \"audio_*_*.wav\")) ) / 300)\n",
    "\n",
    "# ensure path ends in .wav\n",
    "if not output_filename[-4:].lower() == '.wav':\n",
    "    output_filename+='.wav'\n",
    "\n",
    "for i in range(n_audio_batches):\n",
    "    print(f\"Merging audio files {i*300} to {((i+1)*300)-1}... \", end='')\n",
    "    os.system(f'sox {os.path.join(audio_save_path, f\"audio_{i:02}_*.wav\")} -b 16 {os.path.join(audio_save_path, f\"concat_{i:02}.wav\")}')\n",
    "    print(\"Done\")\n",
    "print(f\"Saving output to '{os.path.join(audio_save_path, output_filename)}'... \", end='')\n",
    "os.system(f'sox \"{os.path.join(audio_save_path, \"concat_*.wav\")}\" -b 16 \"{os.path.join(audio_save_path, output_filename)}\"') # merge the merged files into a final output. bit depth of 16 required to go over 4 hour length\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(64.5/1024)/1.29\n",
    "# 0.048828125 MB per second of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.048828125 * (74*3600))*(600//100) / 1024\n",
    "# approx 3811 GB to store all the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice( list(range(0,600,100)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"filename.mel100.npy\".split(\".mel\")[1].split(\".npy\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(r\"D:\\ClipperDatasetV2\\SlicedDialogue\\FiM\\S1\\s1e1\\00_00_05_Celestia_Neutral__Once upon a time.npy\").dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = (np.ones(5) * int(2**31)).astype('int32')\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x.astype(np.float32)/int(2147483648))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "x = [0,1,2,3]\n",
    "y = ['A','B','C','D']\n",
    "z = list(zip(x,y))\n",
    "\n",
    "x, y = zip(*random.Random(1).sample(z, len(z)))\n",
    "x, y = zip(*random.Random(1).sample(z, len(z)))\n",
    "print(type(x))\n",
    "print(x)\n",
    "print(type(y))\n",
    "print(y)\n",
    "print(y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "x = [0,1,2,3]\n",
    "y = ['A','B','C','D']\n",
    "z = list(zip(x,y))\n",
    "\n",
    "x, y = zip(*random.Random(1).sample(z, len(z)))\n",
    "print(type(x))\n",
    "print(x)\n",
    "print(type(y))\n",
    "print(y)\n",
    "print(y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in zip(torch.rand(5), torch.zeros(5)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([0,1],[1,2]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0]*3\n",
    "x[0] = \"blah\"\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str text file\n",
    "# arr split by quotes\n",
    "# arr split by periods and quotes based on length\n",
    "# generator[arr] split by text batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show saved Postnet Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "#filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2\"\n",
    "files = sorted(glob(filepath+\"/**/*.mel.npy\", recursive=True))\n",
    "start = int(random.random() * len(files))\n",
    "file_count = 10\n",
    "for i in range(start, start+file_count):\n",
    "    #file = random.choice(files)\n",
    "    file = files[i]\n",
    "    H = np.load(file)\n",
    "    H = (H+5.2)*0.5\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(file+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "    # again\n",
    "    file = file.replace(\".mel.npy\",\".npy\")\n",
    "    H = np.load(file)\n",
    "    H = (H+5.2)*0.5\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(file+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of GT Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "#filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue/FiM/S1/s1e1\"\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2\"\n",
    "files = sorted(glob(filepath+\"/**/*.npy\", recursive=True))\n",
    "total_size = 0\n",
    "min_duration = 0.6\n",
    "SR = 48000\n",
    "BD = 2 # 16 bits\n",
    "for path in files:\n",
    "    file_size = os.stat(path).st_size\n",
    "    if file_size > (SR*BD*min_duration):\n",
    "        total_size+=file_size\n",
    "\n",
    "duration = total_size / (SR*BD)\n",
    "duration_min = duration/60\n",
    "duration_hrs = duration/3600\n",
    "total_size_MB = total_size / (1024**3)\n",
    "print(f\"{total_size_MB} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size & Duration of Wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "def get_stats(filepath, ext='.wav'):\n",
    "    import soundfile as sf\n",
    "    files = sorted(glob(filepath+f\"/**/*{ext}\", recursive=True))\n",
    "    total_size = 0\n",
    "    total_duration = 0\n",
    "    min_duration = 0.6\n",
    "    SR = 48000\n",
    "    BD = 2 # 16 bits\n",
    "    for path in files:\n",
    "        file_size = os.stat(path).st_size\n",
    "        audio, samplerate = sf.read(path)\n",
    "        if len(audio)/samplerate > min_duration:\n",
    "            total_size+=file_size\n",
    "            total_duration+=len(audio)/samplerate\n",
    "    duration_min = total_duration/60\n",
    "    duration_hrs = total_duration/3600\n",
    "    print(f\"{total_duration} seconds = {duration_min} minutes = {duration_hrs} hours\")\n",
    "    total_size_MB = total_size / (1024**3)\n",
    "    print(f\"{total_size_MB} GB of wavs\")\n",
    "\n",
    "get_stats(r\"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/Blizzard2011\", ext='.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing FP16 vs FP32 individual Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor.inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda().half()\n",
    "#########################################################\n",
    "def b_inv(b_mat):\n",
    "    eye = b_mat.new_ones(b_mat.size(-1)).diag().expand_as(b_mat).float()\n",
    "    b_inv, _ = torch.solve(eye.float(), b_mat.float())\n",
    "    return b_inv\n",
    "fp16 = b_inv(weight.squeeze())\n",
    "#########################################################\n",
    "print(fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda().float()\n",
    "#########################################################\n",
    "fp32 = weight.squeeze().inverse()\n",
    "#########################################################\n",
    "print(fp32)\n",
    "#assert torch.equal(fp32.float(), fp16.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor.slogdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(1000,2,2).cuda().float()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().slogdet()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(1000,2,2).cuda().float()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().logdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(1000,2,2).cuda().float()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().det().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10000\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(500,1,1).cuda().half()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().float().det().log().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10000\n",
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).unsqueeze(0).repeat(500,1,1).cuda().half()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().float().det().half().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda().half()\n",
    "#########################################################\n",
    "log_det_W = weight.squeeze().float().det().half().log()\n",
    "print(log_det_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor([[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]).cuda()\n",
    "weight.half()\n",
    "weight.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Multiple WaveGlow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from denoiser import Denoiser\n",
    "from glob import glob\n",
    "from random import random\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "from layers import TacotronSTFT, STFT\n",
    "from utils import load_wav_to_torch\n",
    "from hparams import create_hparams\n",
    "%matplotlib inline\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def disp_mel(H, desc=''):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(desc+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    #plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "def plot_audio_spec(audio, sampling_rate=48000):\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "\n",
    "\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "\n",
    "\n",
    "def load_model(waveglow_path):\n",
    "    from efficient_model import WaveGlow\n",
    "    from efficient_util import remove_weight_norms\n",
    "    import json\n",
    "    data = r\"\"\"{\n",
    "        \"train_config\": {\n",
    "            \"fp16_run\": false,\n",
    "            \"output_directory\": \"outdir_EfficientBaseline\",\n",
    "            \"epochs\": 1000,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"sigma\": 1.0,\n",
    "            \"iters_per_checkpoint\": 2000,\n",
    "            \"batch_size\": 30,\n",
    "            \"seed\": 1234,\n",
    "            \"checkpoint_path\": \"outdir_EfficientBaseline/waveglow_1425\",\n",
    "            \"with_tensorboard\": true\n",
    "        },\n",
    "        \"data_config\": {\n",
    "            \"training_files\": \"map_0_GT.txt\",\n",
    "            \"segment_length\": 19200,\n",
    "            \"sampling_rate\": 48000,\n",
    "            \"filter_length\": 2400,\n",
    "            \"hop_length\": 600,\n",
    "            \"win_length\": 2400,\n",
    "            \"mel_fmin\": 0.0,\n",
    "            \"mel_fmax\": 16000.0\n",
    "        },\n",
    "        \"dist_config\": {\n",
    "            \"dist_backend\": \"nccl\",\n",
    "            \"dist_url\": \"tcp://127.0.0.1:54321\"\n",
    "        },\n",
    "        \"waveglow_config\": {\n",
    "            \"n_mel_channels\": 160,\n",
    "            \"n_flows\": 12,\n",
    "            \"n_group\": 8,\n",
    "            \"n_early_every\": 4,\n",
    "            \"n_early_size\": 2,\n",
    "            \"memory_efficient\": false,\n",
    "            \"WN_config\": {\n",
    "                \"dilation_channels\":256,\n",
    "                \"residual_channels\":256,\n",
    "                \"skip_channels\":256,\n",
    "                \"n_layers\": 9,\n",
    "                \"radix\": 3,\n",
    "                \"bias\": true\n",
    "            }\n",
    "        }\n",
    "    }\"\"\"\n",
    "    config = json.loads(data)\n",
    "    train_config = config[\"train_config\"]\n",
    "    global data_config\n",
    "    data_config = config[\"data_config\"]\n",
    "    global dist_config\n",
    "    dist_config = config[\"dist_config\"]\n",
    "    global waveglow_config\n",
    "    waveglow_config = { \n",
    "        **config[\"waveglow_config\"], \n",
    "        'win_length': data_config['win_length'],\n",
    "        'hop_length': data_config['hop_length']\n",
    "    }\n",
    "    waveglow = WaveGlow(**waveglow_config).cuda()\n",
    "    waveglow_dict = torch.load(waveglow_path)['model'].state_dict()\n",
    "    waveglow.load_state_dict(waveglow_dict)\n",
    "    waveglow.apply(remove_weight_norms)\n",
    "    waveglow.cuda().eval()#.half()\n",
    "    #for k in waveglow.convinv:\n",
    "    #    k.float()\n",
    "    #denoiser = Denoiser(waveglow)\n",
    "    waveglow_iters = torch.load(waveglow_path)['iteration']\n",
    "    print(waveglow_iters, \"iterations\")\n",
    "    return waveglow, waveglow_iters\n",
    "\n",
    "\n",
    "def waveglow_infer(mel_outputs_postnet, sigma_, iters=''):\n",
    "    current_audio = waveglow.infer(mel_outputs_postnet, sigma=sigma_).unsqueeze(0)\n",
    "    audio.append(current_audio)\n",
    "    print(\"sigma = {}\".format(sigma_)); ipd.display(ipd.Audio(audio[len(audio)-1][0].data.cpu().numpy(), rate=hparams.sampling_rate))\n",
    "    maxv = np.iinfo(np.int16).max\n",
    "    sf.write(\"infer/temp.wav\", (np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate)\n",
    "    disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path+f\"\\nAfter WaveGlow {iters}\\nSigma {sigma_}\")\n",
    "\n",
    "\n",
    "def waveglow_infer_filepath(file_path, sigma_, iters=''):\n",
    "    mel_outputs_postnet = np.load(file_path)\n",
    "    mel_outputs_postnet = (mel_outputs_postnet+5.2)*0.5 # shift values between approx -4 and 4\n",
    "    mel_outputs_postnet = torch.from_numpy(mel_outputs_postnet).unsqueeze(0).cuda()#.half()\n",
    "    audio = []\n",
    "    with torch.no_grad():\n",
    "        waveglow_infer(mel_outputs_postnet, sigma_, iters=iters)\n",
    "\n",
    "audio = []\n",
    "hparams = create_hparams()\n",
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "files = sorted(glob(filepath+\"/**/*.npy\", recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[int(random()*len(files))]\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"outdir_EfficientBaseline/waveglow_5158\n",
    "outdir_EfficientBaseline/waveglow_5483\n",
    "outdir_EfficientBaseline/waveglow_5666\n",
    "outdir_EfficientBaseline/waveglow_7264\n",
    "outdir_EfficientBias/waveglow_252\n",
    "outdir_EfficientBias/waveglow_655\n",
    "outdir_EfficientBias/waveglow_1003\n",
    "outdir_EfficientBias/waveglow_1935\n",
    "outdir_EfficientBias/waveglow_11346\n",
    "outdir_EfficientBias/waveglow_11863\n",
    "outdir_EfficientBias/waveglow_12118\n",
    "outdir_EfficientBias/waveglow_12282\n",
    "outdir_EfficientBias/waveglow_14197\n",
    "outdir_EfficientBias/waveglow_16058\"\"\"\n",
    "model_paths = r\"\"\"\n",
    "outdir_EfficientBias/waveglow_365\n",
    "\"\"\".split(\"\\n\")\n",
    "\n",
    "disp_mel(load_mel(file_path.replace(\".mel.npy\",\".wav\").replace(\".npy\",\".wav\")).squeeze(), desc=file_path.replace(\".mel.npy\",\".wav\").replace(\".npy\",\".wav\")+f\"\\nGround Truth\")\n",
    "for model_path in [path for path in model_paths if path]:\n",
    "    waveglow, iters = load_model(f\"/media/cookie/Samsung 860 QVO/TTCheckpoints/waveglow/{model_path}\")\n",
    "    waveglow_infer_filepath(file_path, 0.9, iters=iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveGlow GTA Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from denoiser import Denoiser\n",
    "from glob import glob\n",
    "from random import random\n",
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "from layers import TacotronSTFT, STFT\n",
    "from utils import load_wav_to_torch\n",
    "from hparams import create_hparams\n",
    "from shutil import copyfile\n",
    "\n",
    "from waveglow_utils import PreEmphasis, InversePreEmphasis\n",
    "%matplotlib inline\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def disp_mel(H, desc=''):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(desc+\"\\nLength: \"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    #plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "def plot_audio_spec(audio, sampling_rate=48000):\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "hparams = create_hparams()\n",
    "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                    hparams.mel_fmax)\n",
    "#stft_wideband = TacotronSTFT(hparams.filter_length, hparams.hop_length, 1024,\n",
    "#                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "#                    hparams.mel_fmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveglow(waveglow_path):\n",
    "    waveglow_dict = torch.load(waveglow_path)\n",
    "    waveglow = waveglow_dict['model']\n",
    "    waveglow_iters = int(waveglow_dict['iteration'])\n",
    "    waveglow.cuda().eval().half()\n",
    "    denoiser = Denoiser(waveglow)\n",
    "    print(waveglow_iters, \"iterations\")\n",
    "    if not hasattr(waveglow, \"spect_scaling\"):\n",
    "        setattr(waveglow, \"spect_scaling\", False)\n",
    "    return waveglow, denoiser, waveglow_iters\n",
    "\n",
    "def load_waveglow_yoyo(waveglow_path):\n",
    "    waveglow_dict = torch.load(waveglow_path)\n",
    "    waveglow = waveglow_dict['model']\n",
    "    waveglow_iters = int(waveglow_dict['iteration'])\n",
    "    waveglow.cuda().eval()#.half()\n",
    "    denoiser = None#Denoiser(waveglow)\n",
    "    print(waveglow_iters, \"iterations\")\n",
    "    return waveglow, denoiser, waveglow_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "waveglow_dict = torch.load(\"/media/cookie/Samsung 860 QVO/TTCheckpoints/waveglow/outdir_EfficientLarge/best_model\")\n",
    "waveglow = waveglow_dict['model']\n",
    "waveglow_iters = int(waveglow_dict['iteration'])\n",
    "waveglow.cuda().eval()#.half()\n",
    "denoiser = None#Denoiser(waveglow)\n",
    "print(waveglow_iters, \"iterations\")\n",
    "print(waveglow.WNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(waveglow.WNs[0].WN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Random File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveglow_infer(mel_outputs_postnet, sigma_, deempathsis, root_dir='infer', clip_folder='', filename=''):\n",
    "    current_audio = waveglow.infer(mel_outputs_postnet, sigma=sigma_)\n",
    "    if len(current_audio.shape) == 1:\n",
    "        current_audio = current_audio.unsqueeze(0)\n",
    "    if deempathsis:\n",
    "        deempthasis_filter = InversePreEmphasis(float(deempathsis)) # TODO, replace with something lightweight.\n",
    "        current_audio = deempthasis_filter(current_audio.cpu().float().unsqueeze(0)).squeeze(0).cuda()\n",
    "    audio.append(current_audio)\n",
    "    \n",
    "    # Show Audio for Listening in Notebook\n",
    "    #ipd.display(ipd.Audio(audio[len(audio)-1][0].data.cpu().numpy(), rate=hparams.sampling_rate))\n",
    "    \n",
    "    # Save Audio\n",
    "    local_fpath = os.path.join(root_dir, clip_folder, filename) # local filepath\n",
    "    local_dpath = os.path.join(root_dir, clip_folder) # local directory path\n",
    "    os.makedirs(local_dpath, exist_ok=True) # ensure local directory exists\n",
    "    maxv = np.iinfo(np.int16).max # get max int16 value\n",
    "    sf.write(os.path.join(root_dir, \"temp.wav\"), (np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate) # write audio to temp\n",
    "    \n",
    "    # Get MSE and MAE\n",
    "    waveglow_spect = load_mel(os.path.join(root_dir, \"temp.wav\")).squeeze() # load spectrogram from wav file.\n",
    "    waveglow_spect_lossy = (waveglow_spect.unsqueeze(0).cuda().half()[:,:,:mel_outputs_postnet.shape[-1]])#+5.2)*0.5 # move spectrogram to GPU, reshape and normalize within -4, 4.\n",
    "    MSE = (nn.MSELoss()(waveglow_spect_lossy, mel_outputs_postnet)).item() # get MSE (Mean Squared Error) between Ground Truth and WaveGlow inferred spectrograms.\n",
    "    MAE = (nn.L1Loss()(waveglow_spect_lossy, mel_outputs_postnet)).item() # get MAE (Mean Absolute Error) between Ground Truth and WaveGlow inferred spectrograms.\n",
    "    \n",
    "    #sf.write(local_fpath+f\"-MSE_{round(MSE,4)}.wav\", (np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate) # write audio to fpath\n",
    "    sf.write(local_fpath+f\"-MSE_{round(MSE,4)}.wav\", np.swapaxes(audio[len(audio)-1].cpu().numpy(),0,1), hparams.sampling_rate, \"PCM_16\") # write audio to fpath\n",
    "    # Show Spect\n",
    "    #disp_mel(waveglow_spect, desc=f\"\\nAfter WaveGlow\\nSigma: {sigma_}\\nMSELoss: {MSE}\") # Show Plot in Notebook\n",
    "    return MSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "files = sorted(glob(filepath+\"/**/*__*.npy\", recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveglow_infer_paths = [\n",
    "    \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue/Special source/s6e24/00_04_52_Rainbow_Neutral__That's a mighty big claim considering everypony here is an amazingly awesome crazy good flyer.mel.npy\"\n",
    "]\n",
    "for i in range(19):\n",
    "    file_path = files[int(random()*len(files))]\n",
    "    waveglow_infer_paths.append(file_path)\n",
    "print(\"\".join([x+\"\\n\" for x in waveglow_infer_paths]))# Print each entry in multiple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"\"\"\n",
    "# Baseline, 12 Flow, 256 Channel, 8 Layer, 0.00 Empthasis\n",
    "outdir_twilight9/waveglow_140900|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_152256|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_187539|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_229258|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_268276|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_311477|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_334361|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_351862|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_387823|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_422080|1|0|0.00|0\n",
    "# Mini (ReZero), 12 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_30000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_38265|1|0|0.97|0\n",
    "# Mini, 12 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_2598|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_3979|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_6496|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_13894|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_15659|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_35439|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_40000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_55133|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_100000|1|0|0.9|07\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_120000|1|0|0.9|07\n",
    "# Mini, 16 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_11306|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_55528|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_57259|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_69333|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_77184|1|0|0.97|0\n",
    "# Mini, 24 Flow, 128 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_66559|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_70000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_80000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_96555|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_108044|1|0|0.97|0\n",
    "# LARGE, 16 Flow, 512 Channel, 10 Layer, 0.97 Empthasis\n",
    "outdir_EfficientLarge/waveglow_60000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_70000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_80000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_90000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_120119|1|0|0.97|1\n",
    "outdir_EfficientLarge/best_model|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_127217|1|0|0.97|1\n",
    "# Baseline (Nancy Datset Only), 12 Flow, 256 Channel, 8 Layer, 0.00 Empthasis\n",
    "\"\"\"\n",
    "\n",
    "# path|normalize(-4 to 4)|mu_law_quantization|de-empthasis|yoyololicon version\n",
    "waveglow_paths = \"\"\"\n",
    "outdir_twilight9/waveglow_140900|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_152256|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_187539|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_229258|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_268276|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_311477|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_334361|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_351862|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_387823|1|0|0.00|0\n",
    "outdir_twilight9/waveglow_422080|1|0|0.00|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_30000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_38265|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_2598|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_3979|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_6496|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_13894|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_15659|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_35439|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_40000|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_55133|1|0|0.97|0\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_100000|1|0|0.9|07\n",
    "outdir_12Flow_128Channel_10Layer/waveglow_120000|1|0|0.9|07\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_11306|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_55528|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_57259|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_69333|1|0|0.97|0\n",
    "outdir_16Flow_128Channel_10Layer/waveglow_77184|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_10000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_66559|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_70000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_80000|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_96555|1|0|0.97|0\n",
    "outdir_24Flow_128Channel_10Layer/waveglow_108044|1|0|0.97|0\n",
    "outdir_EfficientLarge/waveglow_60000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_70000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_80000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_90000|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_120119|1|0|0.97|1\n",
    "outdir_EfficientLarge/best_model|1|0|0.97|1\n",
    "outdir_EfficientLarge/waveglow_127217|1|0|0.97|1\n",
    "outdir_NancyOnly/best_model|0|0|0.00|0\n",
    "\"\"\"[1:-1].split(\"\\n\")\n",
    "\n",
    "print(\"Missing Checkpoints:\")\n",
    "print(\"\\n\".join([x.split(\"|\")[0] for x in waveglow_paths if not os.path.exists(\"../tacotron2/waveglow_latest/\"+x.split(\"|\")[0])]))\n",
    "waveglow_paths = [x for x in waveglow_paths if os.path.exists(\"../tacotron2/waveglow_latest/\"+x.split(\"|\")[0])]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for waveglow_meta in waveglow_paths:\n",
    "        print(\"--------------------------------------------------------\")\n",
    "        waveglow_path, normalize_spec, mu_law_quantization, deempthasis_strength, b_yoyololicon_model = waveglow_meta.split(\"|\")\n",
    "        waveglow_info = str(\"_\".join(waveglow_path.split(\"/\")[0].split(\"_\")[1:]))\n",
    "        print(waveglow_meta)\n",
    "        if b_yoyololicon_model:\n",
    "            waveglow, denoiser, waveglow_current_iter = load_waveglow_yoyo(\"../tacotron2/waveglow_latest/\"+waveglow_path)\n",
    "        else:\n",
    "            waveglow, denoiser, waveglow_current_iter = load_waveglow(\"../tacotron2/waveglow_latest/\"+waveglow_path)\n",
    "        \n",
    "        if not hasattr(waveglow, \"spect_scaling\"):\n",
    "            setattr(waveglow, \"spect_scaling\", False)\n",
    "        \n",
    "        audio = []\n",
    "        total_MAE = total_MSE = 0\n",
    "        best_MAE = best_MSE = 9e9\n",
    "        worst_MAE = worst_MSE = -9e9\n",
    "        for file_path in waveglow_infer_paths:\n",
    "            #print(f\"FILE: {file_path}\") # Print the file path\n",
    "            basename = os.path.splitext(os.path.basename(file_path.replace(\".mel.npy\",\".npy\")))[0] # filename without ext\n",
    "            \n",
    "            mel_outputs_postnet = np.load(file_path) # Load Tacotron2 Postnet Outputs\n",
    "            if int(normalize_spec):\n",
    "                mel_outputs_postnet = (mel_outputs_postnet+5.2)*0.5 # shift values between approx -4 and 4\n",
    "            \n",
    "            #disp_mel(load_mel(file_path.replace(\".mel.npy\",\".wav\").replace(\".npy\",\".wav\")).squeeze(), desc=f\"\\nGround Truth\") # Display Ground Truth Spectrogram\n",
    "            #disp_mel(mel_outputs_postnet, desc=\"\\nThis is the original Postnet output from Tacotron\") # Display Tacotron GTA Spectrogram\n",
    "            \n",
    "            mel_outputs_postnet = np.load(file_path.replace(\".mel.npy\",\".npy\")) # Load Ground Truth Spectrogram for inference by WaveGlow.\n",
    "            if int(normalize_spec):\n",
    "                mel_outputs_postnet = (mel_outputs_postnet+5.2)*0.5 # shift values between approx -4 and 4, speeds up initial training\n",
    "            mel_outputs_postnet = torch.from_numpy(mel_outputs_postnet).unsqueeze(0).cuda() # prep tensor for WaveGlow.\n",
    "            if not b_yoyololicon_model:\n",
    "                mel_outputs_postnet = mel_outputs_postnet.half()\n",
    "            \n",
    "            sigma = 0.9\n",
    "            MSE, MAE = waveglow_infer(mel_outputs_postnet, sigma, deempthasis_strength, clip_folder=f'{basename}', filename=f'iter_{waveglow_current_iter:07}-sigma_{sigma}-empth_{float(deempthasis_strength)}')\n",
    "            sigma = 0.95\n",
    "            MSE, MAE = waveglow_infer(mel_outputs_postnet, sigma, deempthasis_strength, clip_folder=f'{basename}', filename=f'iter_{waveglow_current_iter:07}-sigma_{sigma}-empth_{float(deempthasis_strength)}')\n",
    "            sigma = 1.0\n",
    "            MSE, MAE = waveglow_infer(mel_outputs_postnet, sigma, deempthasis_strength, clip_folder=f'{basename}', filename=f'iter_{waveglow_current_iter:07}-sigma_{sigma}-empth_{float(deempthasis_strength)}')\n",
    "            \n",
    "            if not os.path.exists(os.path.join(\"infer\", basename, f'GroundTruth.wav')):\n",
    "                copyfile(os.path.splitext(file_path.replace(\".mel.npy\",\".npy\"))[0]+\".wav\", os.path.join(\"infer\", basename, f'GroundTruth.wav'))\n",
    "            total_MSE+=MSE\n",
    "            best_MSE = min(best_MSE, MSE)\n",
    "            worst_MSE = max(worst_MSE, MSE)\n",
    "            total_MAE+=MAE\n",
    "            best_MAE = min(best_MAE, MAE)\n",
    "            worst_MAE = max(worst_MAE, MAE)\n",
    "        print(f\"Average MSE: {total_MSE/len(waveglow_infer_paths)} Best MSE: {best_MSE} Worst MSE: {worst_MSE}\")\n",
    "        print(f\"Average MAE: {total_MAE/len(waveglow_infer_paths)} Best MAE: {best_MAE} Worst MAE: {worst_MAE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = torch.rand(4096,1024,2,1, device=\"cuda:0\")\n",
    "x = x + 10\n",
    "x = x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = torch.rand(4096,1024,2,1, device=\"cuda:0\")\n",
    "x = x + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(waveglow, \"spect_scaling\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(waveglow.parameters())[0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dim = 2\n",
    "n_group = 8\n",
    "audio = torch.arange(32).repeat(batch_dim, 1)\n",
    "print(audio)\n",
    "audio = audio.view(batch_dim, -1, n_group).transpose(1, 2)\n",
    "print(\"shape =\", audio.shape)\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,6,1)\n",
    "print(x)\n",
    "y_0, y_1 = x.chunk(2,1)\n",
    "print(\"y_0 =\\n\", y_0)\n",
    "print(\"y_1 =\\n\", y_1)\n",
    "\n",
    "n_half = int(x.size(1)/2)\n",
    "y_0 = x[:,:n_half,:]\n",
    "y_1 = x[:,n_half:,:]\n",
    "print(\"y_0 =\\n\", y_0)\n",
    "print(\"y_1 =\\n\", y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)\n",
    "print(x)\n",
    "x.mul_(2).add_(-5)\n",
    "print(x.view(-1))\n",
    "x = x.unsqueeze(0)\n",
    "print(x.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (torch.rand(5)-0.5)*2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x*x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.abs(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\n",
    "    torch.cuda.get_device_properties(\"cuda:0\").total_memory/1024**3,\"\\n\",\n",
    "    torch.cuda.memory_allocated(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.max_memory_allocated(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.memory_reserved(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.max_memory_reserved(\"cuda:0\")/1024**3,\"\\n\",\n",
    "    torch.cuda.memory_summary(\"cuda:0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoise_strength = 10\n",
    "for i in audio:\n",
    "    audio_denoised = denoiser(i, strength=denoise_strength)[:, 0]\n",
    "    ipd.display(ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate))\n",
    "maxv = np.iinfo(np.int16).max\n",
    "sf.write(\"infer/temp.wav\", (np.swapaxes(audio_denoised.cpu().numpy(),0,1) * maxv).astype(np.int16), hparams.sampling_rate)\n",
    "disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path+f\"\\nAfter Denoise\\nDenoise Strength {denoise_strength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,5)\n",
    "print(x)\n",
    "y = x*torch.tensor([0,1,0,1,0])\n",
    "print(y)\n",
    "# masked_fill_ should be a little more performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "truncated_length = 100\n",
    "lengths = torch.tensor([268, 239, 296, 148, 87, 453, 601, 602, 603, 604, 605, 606, 607, 608, 609])\n",
    "processed = 0\n",
    "\n",
    "batch_lengths = lengths[:batch_size]\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"-\"*100)\n",
    "    print(batch_lengths)\n",
    "    print((batch_lengths-truncated_length))\n",
    "    print((batch_lengths-truncated_length)[batch_lengths-truncated_length>0])\n",
    "    print((batch_lengths-truncated_length)[batch_lengths-truncated_length>0].shape[0])\n",
    "    print(batch_size - (batch_lengths-truncated_length)[batch_lengths-truncated_length>0].shape[0])\n",
    "    \n",
    "    #batch_lengths = (batch_lengths-truncated_length)[batch_lengths-truncated_length>0]\n",
    "    print(batch_lengths)\n",
    "    print(\"processed =\",processed)\n",
    "    processed+=batch_size-((batch_lengths-truncated_length)[batch_lengths-truncated_length>0]).shape[0]\n",
    "    batch_lengths = torch.cat((batch_lengths, lengths[processed+batch_size:processed+batch_size+(batch_size-batch_lengths.shape[0])]), 0)\n",
    "    print(batch_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "processed = 0\n",
    "lengths = torch.tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "x = lengths[:batch_size]\n",
    "print(x)\n",
    "print(x[x<4].shape[0])\n",
    "x[x<4] = lengths[processed+batch_size:processed+batch_size+x[x<4].shape[0]]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Pre-empthasis for Audio Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "from random import random\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from waveglow_utils import PreEmphasis, InversePreEmphasis\n",
    "preempthasis_strength = 0.97\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue\"\n",
    "files = sorted(glob(filepath+\"/**/*__*.wav\", recursive=True))\n",
    "\n",
    "filepath = \"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/VCTK-Corpus-0.92/wav\"\n",
    "files = sorted(glob(filepath+\"/**/*.wav\", recursive=True))\n",
    "\n",
    "preempth_filter = PreEmphasis(preempthasis_strength).float()\n",
    "deempth_filter = InversePreEmphasis(preempthasis_strength).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[int(random()*len(files))]\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[int(random()*len(files))]\n",
    "print(file_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio, sample_rate = sf.read(file_path)\n",
    "    print(audio.max())\n",
    "    print(audio.min())\n",
    "    print(\"Original\")\n",
    "    ipd.display(ipd.Audio(audio, rate=sample_rate))\n",
    "    \n",
    "    sf.write(\"infer/temp_original.wav\", audio, sample_rate)\n",
    "    disp_mel(load_mel(\"infer/temp_original.wav\").squeeze(), desc=file_path)\n",
    "    \n",
    "    import scipy\n",
    "    from scipy import signal\n",
    "    sos = signal.butter(10, 60, 'hp', fs=48000, output='sos')\n",
    "    filtered_audio = signal.sosfilt(sos, audio)\n",
    "    \n",
    "    sf.write(\"infer/temp.wav\", filtered_audio, sample_rate)\n",
    "    disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path)\n",
    "    \n",
    "    sos = signal.butter(2, 60, 'hp', fs=48000, output='sos')\n",
    "    filtered_audio = signal.sosfilt(sos, audio)\n",
    "    \n",
    "    sf.write(\"infer/temp.wav\", filtered_audio, sample_rate)\n",
    "    disp_mel(load_mel(\"infer/temp.wav\").squeeze(), desc=file_path)\n",
    "    \n",
    "    maxv = np.iinfo(np.int16).max\n",
    "    audio = deempth_filter((torch.tensor(audio)/maxv).unsqueeze(0).unsqueeze(0).float())\n",
    "    print(audio.mean())\n",
    "    print(audio.std())\n",
    "    print(\"De-Empthasis\")\n",
    "    ipd.display(ipd.Audio(audio.squeeze()*maxv, rate=sample_rate))\n",
    "    \n",
    "    audio, sample_rate = sf.read(file_path)\n",
    "    audio = preempth_filter((torch.tensor(audio)/maxv).unsqueeze(0).unsqueeze(0).float())\n",
    "    print(\"Pre-Empthasis\")\n",
    "    ipd.display(ipd.Audio(audio.squeeze()*maxv, rate=sample_rate))\n",
    "    \n",
    "    print(\"Pre-Empthasis + De-Empthasis\")\n",
    "    audio = deempth_filter(audio)\n",
    "    ipd.display(ipd.Audio(audio.squeeze()*maxv, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mcd(C, C_hat):\n",
    "    \"\"\"C and C_hat are NumPy arrays of shape (T, D),\n",
    "    representing mel-cepstral coefficients.\n",
    "\n",
    "    \"\"\"\n",
    "    K = 10 / np.log(10) * np.sqrt(2)\n",
    "    return K * np.mean(np.sqrt(np.sum((C - C_hat) ** 2, axis=1)))\n",
    "mcd(np.array([[0,0.8,0]]),np.array([[0,1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Force Loading Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = {\n",
    "    'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "    'e':5,\n",
    "}\n",
    "modeldict = {\n",
    "    'c':3,\n",
    "    'e':4,\n",
    "    'f':5,\n",
    "}\n",
    "dummy_modeldict = {k: v for k,v in pretrained.items() if k in modeldict and pretrained[k] == modeldict[k]}\n",
    "model_dict_missing = {k: v for k,v in pretrained.items() if k not in modeldict}\n",
    "model_dict_mismatching = {k: v for k,v in pretrained.items() if k in modeldict and pretrained[k] != modeldict[k]}\n",
    "pretrained_missing = {k: v for k,v in modeldict.items() if k not in pretrained}\n",
    "print(list(model_dict_missing.keys()),'does not exist in the current model')\n",
    "print(list(model_dict_mismatching.keys()),\"is the wrong shape and has been reset\")\n",
    "print(list(pretrained_missing.keys()),\"doesn't have pretrained weights and is reset\")\n",
    "print(dummy_modeldict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Blur to Spectrograms during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio / hparams.max_wav_value\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec\n",
    "def disp_mel(H, desc=''):\n",
    "    %matplotlib inline\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(desc+\"\\n\"+str(round(np.size(H,1)*(600/48000), 2)))\n",
    "    plt.imshow(H, cmap='inferno', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    #plt.colorbar(orientation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = load_mel(\"/media/cookie/Samsung 860 QVO/ClipperDatasetV2/SlicedDialogue/Special source/s6e24/00_03_22_Rainbow_Neutral__tell ya what. I'll leave the teaching stuff to you And I'll just make sure they stay awake.wav\")\n",
    "scale_embed = torch.rand([1,160])*0.5 + 0.55\n",
    "x = x*scale_embed.unsqueeze(2)\n",
    "print(x.shape)\n",
    "print(scale_embed.shape)\n",
    "print(scale_embed)\n",
    "x = x.cuda()\n",
    "disp_mel(x.squeeze().cpu())\n",
    "\n",
    "x_strength = 100.0\n",
    "y_strength = 0.001\n",
    "filter_cycles = 3\n",
    "filter = kornia.filters.GaussianBlur2d((3,3),(x_strength,y_strength))\n",
    "\n",
    "#print(x.shape)\n",
    "# filter input needs (B,C,H,W)\n",
    "out = filter(x.unsqueeze(0))\n",
    "for i in range(filter_cycles-1): out = filter(out)\n",
    "disp_mel(out.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask for Tacotron Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_mask_from_lengths(lengths):\n",
    "    max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, out=torch.LongTensor(max_len))\n",
    "    mask = (ids < lengths.unsqueeze(1))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4,8)\n",
    "output_lengths = torch.tensor([8,5,2,1])\n",
    "print(x)\n",
    "print(output_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ~get_mask_from_lengths(output_lengths)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.expand_as(x)\n",
    "print(y)\n",
    "x.masked_fill_(y, 1e3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
